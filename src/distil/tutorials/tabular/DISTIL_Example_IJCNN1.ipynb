{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using DISTIL for Tabular Data (IJCNN1).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuGGJTKLK0SU"
      },
      "source": [
        "# **DISTIL Usage Example: IJCNN1**\n",
        "\n",
        "Here, we show how to use DISTIL to perform active learning on tabular data (IJCNN1). This notebook can be easily executed on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW2wJKUoL1I-"
      },
      "source": [
        "## Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iiAc1qWL5Db"
      },
      "source": [
        "# Get DISTIL\n",
        "!git clone https://github.com/decile-team/distil.git\n",
        "!pip install -r distil/requirements/requirements.txt\n",
        "\n",
        "# Get IJCNN1 dataset, which is kept in our datasets repository\n",
        "!git clone https://github.com/decile-team/datasets.git\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "sys.path.append('distil/')\n",
        "from distil.utils.utils import LabeledToUnlabeledDataset          # Converts a PyTorch dataset with labels to one without labels\n",
        "from distil.active_learning_strategies.glister import GLISTER     # Our choice of active learning strategy for this example\n",
        "from distil.utils.models.simple_net import TwoLayerNet            # Our choice of model for this example\n",
        "from distil.utils.train_helper import data_train                  # The training loop used in between AL selections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opnHILkQPta8"
      },
      "source": [
        "## Preparing IJCNN1\n",
        "\n",
        "The data for IJCNN1 is prepared in this step. Here, we load the data, normalize it, partition the train dataset into a labeled seed set and unlabeled set, and formulate PyTorch datasets. The main output of this step is to create the PyTorch dataset objects used in training and by DISTIL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qwO5cWhJ4Ie"
      },
      "source": [
        "def libsvm_file_load(path,dim, save_data=False):\n",
        "    \"\"\"\n",
        "    Used to load IJCNN. Returns a tuple of numpy arrays representing \n",
        "    the tabular features and their corresponding labels.\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "    target = []\n",
        "    with open(path) as fp:\n",
        "       line = fp.readline()\n",
        "       while line:\n",
        "        temp = [i for i in line.strip().split(\" \")]\n",
        "        target.append(int(float(temp[0]))) # Class Number. # Not assumed to be in (0, K-1)\n",
        "        temp_data = [0]*dim\n",
        "        \n",
        "        for i in temp[1:]:\n",
        "            ind,val = i.split(':')\n",
        "            temp_data[int(ind)-1] = float(val)\n",
        "        data.append(temp_data)\n",
        "        line = fp.readline()\n",
        "    X_data = np.array(data,dtype=np.float32)\n",
        "    Y_label = np.array(target)\n",
        "    if save_data:\n",
        "        # Save the numpy files to the folder where they come from\n",
        "        data_np_path = path + '.data.npy'\n",
        "        target_np_path = path + '.label.npy'\n",
        "        np.save(data_np_path, X_data)\n",
        "        np.save(target_np_path, Y_label)\n",
        "    return (X_data, Y_label)\n",
        "\n",
        "# Specify locations of IJCNN1's train/val/test data\n",
        "trn_file = 'datasets/ijcnn1/ijcnn1.trn'\n",
        "val_file = 'datasets/ijcnn1/ijcnn1.val'\n",
        "tst_file = 'datasets/ijcnn1/ijcnn1.tst'\n",
        "\n",
        "# IJCNN1 has 22 input features and a binary label\n",
        "data_dims = 22\n",
        "num_cls = 2\n",
        "\n",
        "# Retrieve numpy arrays for each part of the dataset\n",
        "x_trn, y_trn = libsvm_file_load(trn_file, dim=data_dims)\n",
        "x_val, y_val = libsvm_file_load(val_file, dim=data_dims)\n",
        "x_tst, y_tst = libsvm_file_load(tst_file, dim=data_dims)\n",
        "    \n",
        "# The class labels are (-1,1). Transform them to (0,1).\n",
        "y_trn[y_trn < 0] = 0\n",
        "y_val[y_val < 0] = 0\n",
        "y_tst[y_tst < 0] = 0    \n",
        "\n",
        "# Normalize the data according to mean/std taken from train dataset\n",
        "sc = StandardScaler()\n",
        "x_trn = sc.fit_transform(x_trn)\n",
        "x_val = sc.transform(x_val)\n",
        "x_tst = sc.transform(x_tst)\n",
        "\n",
        "# Record the number of samples in train dataset\n",
        "nSamps, dim = np.shape(x_trn)\n",
        "\n",
        "# Randomly choose indices of the train dataset on which to split\n",
        "np.random.seed(42)\n",
        "start_idxs = np.random.choice(nSamps, size=32, replace=False)\n",
        "\n",
        "# Split the features into labeled seed set features and unlabeled set features\n",
        "X_tr = x_trn[start_idxs]\n",
        "X_unlabeled = np.delete(x_trn, start_idxs, axis = 0)\n",
        "\n",
        "# Split the labels into labeled seed set labels and unlabeled set labels (which are typically not known a priory; however, we use them to automatically label points)\n",
        "y_tr = y_trn[start_idxs]\n",
        "y_unlabeled = np.delete(y_trn, start_idxs, axis = 0)\n",
        "\n",
        "# Lastly, we create the PyTorch dataset objects. Here, the unlabeled dataset technically has labels;\n",
        "# however, we will explicitly remove these labels when it is used by DISTIL's active learning strategy.\n",
        "# It only contains the labels in this notebook for the sake of experimental design.\n",
        "training_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr, dtype=torch.long))\n",
        "unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled), torch.tensor(y_unlabeled, dtype=torch.long))\n",
        "test_dataset = TensorDataset(torch.tensor(x_tst), torch.tensor(y_tst, dtype=torch.long))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCO6jUYoQ1-S"
      },
      "source": [
        "## Preparing the Model\n",
        "\n",
        "Here, we use DISTIL's two-layer network, which consists of a hidden layer of ReLU activations. We specify the input dimension via the first argument, the number of output classes via the second argument, and the number of hidden units via the third argument. The network then has its weights initialized. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkiY_qXzQ2SO"
      },
      "source": [
        "def init_weights(m):\n",
        "    \"\"\"\n",
        "    Used to initialize network weights\n",
        "    \"\"\"\n",
        "\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "net = TwoLayerNet(dim, num_cls,100)\n",
        "net.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE5uH8TQSMYG"
      },
      "source": [
        "## Defining the Active Learning Strategy\n",
        "\n",
        "We now have all that we need to create the active learning strategy object. For this example, we use [GLISTER-ACTIVE](https://arxiv.org/abs/2012.10630). The GLISTER strategy takes the current labeled dataset (training_dataset), the current unlabeled dataset (unlabeled_dataset, which has its labels stripped via the LabeledToUnlabeledDataset wrapper), the model (net), the number of classes (num_cls), various strategy arguments (strategy_args), a validation dataset (which we set to none), the type of regularization that GLISTER uses (typeOf), and the corresponding regularization coefficient (lam)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9ON0t61SMqc"
      },
      "source": [
        "# Define the strategy args for GLISTER, which needs the learning rate used in training.\n",
        "# We also specify a specific batch size that should be used when loading/handling data \n",
        "# within the strategy.\n",
        "strategy_args = {'batch_size' : 100, 'lr':float(0.001)} \n",
        "strategy = GLISTER(training_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, num_cls, strategy_args, validation_dataset = None, typeOf='Diversity', lam=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLRHPlivTXFH"
      },
      "source": [
        "## Perform the AL Loop\n",
        "\n",
        "We can now begin the active learning loop. Here, we define our training loop through DISTIL's utility training loop. We continuously select points using GLISTER, label them, add them to the train dataset, retrain the model, and repeat for a certain number of rounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ0IJ9g9TXeT"
      },
      "source": [
        "# Define the training loop arguments. Here, we specify that training should stop after \n",
        "# 150 epochs. Internally, the training class also stops at 0.95 training accuracy. This \n",
        "# can be changed by specifying a max_accuracy parameter here.\n",
        "train_args = {'n_epoch':150, 'lr':float(0.001)}\n",
        "n_rounds = 10     # Number of active learning rounds\n",
        "budget = 32       # The budget of AL selection; defines how many points should be retrieved from the unlabeled dataset.\n",
        "\n",
        "# Keep track of the test accuracy obtained at each round to measure progress\n",
        "acc = np.zeros(n_rounds)\n",
        "\n",
        "# Create the training loop class.\n",
        "dt = data_train(training_dataset, net, train_args)\n",
        "\n",
        "# Do one round of training. To make accurate selections, the model must at least be trained on the seed set data.\n",
        "clf = dt.train()\n",
        "\n",
        "# Update the active learning strategy's stored model.\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Use the active learning strategy's predict() method to obtain model predictions on the test features.\n",
        "# Obtain initial test accuracy using predictions.\n",
        "y_pred = strategy.predict(LabeledToUnlabeledDataset(test_dataset)).cpu().numpy()\n",
        "acc[0] = (1.0*(y_tst == y_pred)).sum().item() / len(y_tst)\n",
        "print('Initial Testing accuracy:', round(acc[0], 3), flush=True)\n",
        "\n",
        "# User-Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # The main functionality of the active learning class: the select() function.\n",
        "    # It retrieves the indices of points in the unlabeled set that should be labeled \n",
        "    # and added to the training set.\n",
        "    idx = strategy.select(budget)\n",
        "    print('New data points added -', len(idx))\n",
        "\n",
        "    # Add the new points to the training set. Here, we do so by modifying the underlying\n",
        "    # numpy arrays. Here, the selected features are concatenated to the training set \n",
        "    # features.\n",
        "    X_tr = np.concatenate((X_tr, X_unlabeled[idx]), axis=0)\n",
        "    X_unlabeled = np.delete(X_unlabeled, idx, axis = 0)\n",
        "\n",
        "    # Here, we concatenate the labels of the selected point to the labels of teh training set.\n",
        "    # This step is done by the human in actual applications; here, it is done automatically \n",
        "    # via our a priori knowledge.\n",
        "    y_tr = np.concatenate((y_tr, y_unlabeled[idx]), axis = 0)\n",
        "    y_unlabeled = np.delete(y_unlabeled, idx, axis = 0)\n",
        "    print('Number of training points -',X_tr.shape[0])\n",
        "    print('Number of labels -', y_tr.shape[0])\n",
        "    print('Number of unlabeled points -', X_unlabeled.shape[0])\n",
        "\n",
        "    # Update the PyTorch dataset objects.\n",
        "    training_dataset = TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr, dtype=torch.long))\n",
        "    unlabeled_dataset = TensorDataset(torch.tensor(X_unlabeled), torch.tensor(y_unlabeled, dtype=torch.long))\n",
        "\n",
        "    # Update the data used in the active learning strategy and the training loop\n",
        "    strategy.update_data(training_dataset, LabeledToUnlabeledDataset(unlabeled_dataset))\n",
        "    dt.update_data(training_dataset)\n",
        "\n",
        "    # Retrain the model using the new labeled data. Update the active learning strategy using \n",
        "    # the newly trained model.\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get the test accuracy as before.\n",
        "    y_pred = strategy.predict(LabeledToUnlabeledDataset(test_dataset)).cpu().numpy()\n",
        "    acc[rd] = round(1.0 * (y_tst == y_pred).sum().item() / len(y_tst), 3)\n",
        "    print('Testing accuracy:', acc[rd], flush=True)\n",
        "\n",
        "    # We add an additional condition here to stop once test accuracy exceeds 0.98. Ideally,\n",
        "    # you'd want to stop at a target test accuracy, anyways.\n",
        "    if acc[rd] > 0.98:\n",
        "        print('Testing accuracy reached above 98%, stopping training!')\n",
        "        break\n",
        "        \n",
        "print('Training Completed')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
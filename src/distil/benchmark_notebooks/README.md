## Active Learning Benchmarking using DISTIL
We include a thorough benchmarking of various AL algorithms that covers many evaluation facets. Our experiments can be found in the benchmark_notebooks folder. To execute these experiments, upload a selected experiment to Google Colab and connect to a hosted runtime. We present the results of this benchmark in each subfolder. More details can be found in [Effective Evaluation of Deep Active Learning on Image Classification Tasks](https://arxiv.org/abs/2106.15324).

## Folder Descriptions

| Folder | Description |
| :--- | :----: |
| [augmentation](augmentation) | Contains notebooks that examine the effect that data augmentation has on the evolution of test accuracy and labeling efficiency in AL. We find that image classification tasks that admit augmentations stand to benefit greatly from their usage in active learning, giving higher test accuracies and labeling efficiency. |
| [baseline](baseline) | Contains notebooks that examine the evolution of test accuracy and labeling efficiency of many AL algorithms in baseline settings. We find that most algorithms are more label-efficient than random sampling, among a couple other takeaways. |
| [budget](budget) | Contains notebooks that examine the effect that the AL budget has on the evolution of test accuracy in AL. We find that reasonable choices of the budget do not greatly affect the evolution of test accuracy among our tested algorithms. |
| [ex_per_class](ex_per_class) | Contains notebooks that examine the effect of the number of unlabeled examples per class on the evolution of test accuracy and labeling efficiency in AL. We find that reducing the number of unlabeled examples per class tends to diminish the benefit of AL. |
| [gradmatch](gradmatch) | Contains a notebook that examines the effect of using DSS techniques for faster training on the evolution of test accuracy in AL. We find that DSS techniques can be used to speed up the training of some AL rounds without performance degradation. |
| [optimizer](optimizer) | Contains notebooks that examine the effect of using adaptive optimizers such as [Adam](https://arxiv.org/abs/1412.6980) on the evolution of test accuracy and labeling efficiency in AL. We find that using Adam achieves slightly lower test accuracies and labeling efficiencies than using SGD. |
| [redundancy](redundancy) | Contains notebooks that examine the effect of redundancy in the unlabeled set on the evolution of test accuracy and labeling efficiency across a few algorithms in AL. We find that AL selection strategies that do not account for diversity are detrimental to the labeling efficiency of the AL process while those that do are robust against such performance degradation. |
| [seed](seed) | Contains notebooks that examine the effect of the type of seed set used (randomly selected v. more carefully constructed) on the evolution of test accuracy in AL. We find that the initial benefit of carefully curating a seed set dimishes after only a couple rounds of AL. |
| [swa_ss](swa_ss) | Contains notebooks that examine the effect of the use of generalization techniques in deep learning on the evolution of test accuracy and labeling efficiency across a few algorithms in AL. We find that these techniques positively contribute to the evolution of test accuracy and labeling efficiency. |
| [warm_start](warm_start) | Contains notebooks that examine the effect of maintaining previously learned models as warm-starts for the next AL round on the evolution of test accuracy in AL. We find that retraining the models from scratch is marginally better than keeping the model from the previous round when using SGD and augmentations; however, we find that this gap is much, much more pronounced when using Adam and no augmentations. |
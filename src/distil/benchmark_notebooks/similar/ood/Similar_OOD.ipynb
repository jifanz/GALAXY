{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4YlT-8B8lLN"
   },
   "source": [
    "# SMI AL Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMIAA-Ua8lLR",
    "outputId": "c379d728-9870-4fca-cbca-24658e0a12ef"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from matplotlib import pyplot as plt\n",
    "from distil.distil.utils.models.resnet import ResNet18\n",
    "from trust.trust.utils.custom_dataset import load_dataset_custom\n",
    "from torch.utils.data import Subset\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "from math import floor\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "from distil.distil.active_learning_strategies.scmi import SCMI\n",
    "from distil.distil.active_learning_strategies.smi import SMI\n",
    "from distil.distil.active_learning_strategies.badge import BADGE\n",
    "from distil.distil.active_learning_strategies.entropy_sampling import EntropySampling\n",
    "from distil.distil.active_learning_strategies.gradmatch_active import GradMatchActive\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "from distil.distil.utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClNjNvIX8lLT"
   },
   "outputs": [],
   "source": [
    "def model_eval_loss(data_loader, model, criterion):\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def init_weights(m):\n",
    "#     torch.manual_seed(35)\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "def weight_reset(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "                \n",
    "def create_model(name, num_cls, device, embedding_type):\n",
    "    if name == 'ResNet18':\n",
    "        if embedding_type == \"gradients\":\n",
    "            model = ResNet18(num_cls)\n",
    "        else:\n",
    "            model = models.resnet18()\n",
    "    elif name == 'MnistNet':\n",
    "        model = MnistNet()\n",
    "    elif name == 'ResNet164':\n",
    "        model = ResNet164(num_cls)\n",
    "    model.apply(init_weights)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "def loss_function():\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
    "    return criterion, criterion_nored\n",
    "\n",
    "def optimizer_with_scheduler(model, num_epochs, learning_rate, m=0.9, wd=5e-4):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def optimizer_without_scheduler(model, learning_rate, m=0.9, wd=5e-4):\n",
    "#     optimizer = optim.Adam(model.parameters(),weight_decay=wd)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                          momentum=m, weight_decay=wd)\n",
    "    return optimizer\n",
    "\n",
    "def generate_cumulative_timing(mod_timing):\n",
    "    tmp = 0\n",
    "    mod_cum_timing = np.zeros(len(mod_timing))\n",
    "    for i in range(len(mod_timing)):\n",
    "        tmp += mod_timing[i]\n",
    "        mod_cum_timing[i] = tmp\n",
    "    return mod_cum_timing/3600\n",
    "            \n",
    "def find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, \n",
    "                       final_tst_predictions, saveDir, prefix):\n",
    "    #find queries from the validation set that are erroneous\n",
    "#     saveDir = os.path.join(saveDir, prefix)\n",
    "#     if(not(os.path.exists(saveDir))):\n",
    "#         os.mkdir(saveDir)\n",
    "    val_err_idx = list(np.where(np.array(final_val_classifications) == False)[0])\n",
    "    tst_err_idx = list(np.where(np.array(final_tst_classifications) == False)[0])\n",
    "    val_class_err_idxs = []\n",
    "    tst_err_log = []\n",
    "    val_err_log = []\n",
    "    for i in range(num_cls):\n",
    "        if(feature==\"ood\"): tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        if(feature==\"classimb\"): tst_class_idxs = list(torch.where(torch.Tensor(test_set.targets) == i)[0].cpu().numpy())\n",
    "        val_class_idxs = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "        #err classifications per class\n",
    "        val_err_class_idx = set(val_err_idx).intersection(set(val_class_idxs))\n",
    "        tst_err_class_idx = set(tst_err_idx).intersection(set(tst_class_idxs))\n",
    "        if(len(val_class_idxs)>0):\n",
    "            val_error_perc = round((len(val_err_class_idx)/len(val_class_idxs))*100,2)\n",
    "        else:\n",
    "            val_error_perc = 0\n",
    "        tst_error_perc = round((len(tst_err_class_idx)/len(tst_class_idxs))*100,2)\n",
    "        print(\"val, test error% for class \", i, \" : \", val_error_perc, tst_error_perc)\n",
    "        val_class_err_idxs.append(val_err_class_idx)\n",
    "        tst_err_log.append(tst_error_perc)\n",
    "        val_err_log.append(val_error_perc)\n",
    "    tst_err_log.append(sum(tst_err_log)/len(tst_err_log))\n",
    "    val_err_log.append(sum(val_err_log)/len(val_err_log))\n",
    "    return tst_err_log, val_err_log, val_class_err_idxs\n",
    "\n",
    "\n",
    "def aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, augrandom=False):\n",
    "    all_lake_idx = list(range(len(lake_set)))\n",
    "    if(not(len(subset)==budget) and augrandom):\n",
    "        print(\"Budget not filled, adding \", str(int(budget) - len(subset)), \" randomly.\")\n",
    "        remain_budget = int(budget) - len(subset)\n",
    "        remain_lake_idx = list(set(all_lake_idx) - set(subset))\n",
    "        random_subset_idx = list(np.random.choice(np.array(remain_lake_idx), size=int(remain_budget), replace=False))\n",
    "        subset += random_subset_idx\n",
    "    lake_ss = SubsetWithTargets(true_lake_set, subset, torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "    if(feature==\"ood\"): \n",
    "        ood_lake_idx = list(set(lake_subset_idxs)-set(subset))\n",
    "        private_set =  SubsetWithTargets(true_lake_set, ood_lake_idx, torch.Tensor(np.array([split_cfg['num_cls_idc']]*len(ood_lake_idx))).float())\n",
    "    remain_lake_idx = list(set(all_lake_idx) - set(lake_subset_idxs))\n",
    "    remain_lake_set = SubsetWithTargets(lake_set, remain_lake_idx, torch.Tensor(lake_set.targets.float())[remain_lake_idx])\n",
    "    remain_true_lake_set = SubsetWithTargets(true_lake_set, remain_lake_idx, torch.Tensor(true_lake_set.targets.float())[remain_lake_idx])\n",
    "    print(len(lake_ss),len(remain_lake_set),len(lake_set))\n",
    "    if(feature!=\"ood\"): assert((len(lake_ss)+len(remain_lake_set))==len(lake_set))\n",
    "    aug_train_set = torch.utils.data.ConcatDataset([train_set, lake_ss])\n",
    "    if(feature==\"ood\"): \n",
    "        return aug_train_set, remain_lake_set, remain_true_lake_set, private_set, lake_ss\n",
    "    else:\n",
    "        return aug_train_set, remain_lake_set, remain_true_lake_set, lake_ss\n",
    "                        \n",
    "def getQuerySet(val_set, val_class_err_idxs, imb_cls_idx, miscls):\n",
    "    miscls_idx = []\n",
    "    if(miscls):\n",
    "        for i in range(len(val_class_err_idxs)):\n",
    "            if i in imb_cls_idx:\n",
    "                miscls_idx += val_class_err_idxs[i]\n",
    "        print(\"total misclassified ex from imb classes: \", len(miscls_idx))\n",
    "    else:\n",
    "        for i in imb_cls_idx:\n",
    "            imb_cls_samples = list(torch.where(torch.Tensor(val_set.targets.float()) == i)[0].cpu().numpy())\n",
    "            miscls_idx += imb_cls_samples\n",
    "        print(\"total samples from imb classes as targets: \", len(miscls_idx))\n",
    "    return Subset(val_set, miscls_idx)\n",
    "\n",
    "\n",
    "def getPrivateSet(lake_set, subset, private_set):\n",
    "    #augment prev private set and current subset\n",
    "    new_private_set = SubsetWithTargets(lake_set, subset, torch.Tensor(lake_set.targets.float())[subset])\n",
    "#     new_private_set =  Subset(lake_set, subset)\n",
    "    total_private_set = torch.utils.data.ConcatDataset([private_set, new_private_set])\n",
    "    return total_private_set\n",
    "\n",
    "def remove_ood_points(lake_set, subset, idc_idx):\n",
    "    idx_subset = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in idc_idx:\n",
    "        idc_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        idx_subset += list(np.array(subset)[idc_subset_idx])\n",
    "    print(len(idx_subset),\"/\",len(subset), \" idc points.\")\n",
    "    return idx_subset\n",
    "\n",
    "def getPerClassSel(lake_set, subset, num_cls):\n",
    "    perClsSel = []\n",
    "    subset_cls = torch.Tensor(lake_set.targets.float())[subset]\n",
    "    for i in range(num_cls):\n",
    "        cls_subset_idx = list(torch.where(subset_cls == i)[0].cpu().numpy())\n",
    "        perClsSel.append(len(cls_subset_idx))\n",
    "    return perClsSel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkbsfjml8lLX",
    "outputId": "283a4f46-1575-40a6-e915-88d2d7a4e793"
   },
   "outputs": [],
   "source": [
    "feature = \"ood\"\n",
    "device_id = 0\n",
    "run=\"fkna_3\"\n",
    "datadir = 'data/'\n",
    "data_name = 'cifar10'\n",
    "model_name = 'ResNet18'\n",
    "num_rep = 10\n",
    "learning_rate = 0.01\n",
    "num_runs = 1  # number of random runs\n",
    "computeClassErrorLog = True\n",
    "\n",
    "magnification = 1\n",
    "device = \"cuda:\"+str(device_id) if torch.cuda.is_available() else \"cpu\"\n",
    "datkbuildPath = \"./datk/build\"\n",
    "exePath = \"cifarSubsetSelector\"\n",
    "print(\"Using Device:\", device)\n",
    "doublePrecision = True\n",
    "linearLayer = True\n",
    "miscls = False\n",
    "# handler = DataHandler_CIFAR10\n",
    "augTarget = True\n",
    "embedding_type = \"gradients\"\n",
    "\n",
    "if(feature==\"ood\"):\n",
    "    num_cls=8\n",
    "    budget=250\n",
    "    num_epochs = int(10)\n",
    "    split_cfg = {'num_cls_idc':8, 'per_idc_train':200, 'per_idc_val':10, 'per_idc_lake':500, 'per_ood_train':0, 'per_ood_val':0, 'per_ood_lake':5000}#cifar10\n",
    "    # split_cfg = {'num_cls_idc':50, 'per_idc_train':100, 'per_idc_val':2, 'per_idc_lake':100, 'per_ood_train':0, 'per_ood_val':0, 'per_ood_lake':500}#cifar100\n",
    "    initModelPath = \"weights/\" + data_name + \"_\" + feature + \"_\" + model_name + \"_\" + str(learning_rate) + \"_\" + str(split_cfg[\"per_idc_train\"]) + \"_\" + str(split_cfg[\"per_idc_val\"]) + \"_\" + str(split_cfg[\"num_cls_idc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qKXzKtd8lLZ"
   },
   "source": [
    "# AL Like Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMSfzzqM8lLZ"
   },
   "outputs": [],
   "source": [
    "def train_model_al(datkbuildPath, exePath, num_epochs, dataset_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run,\n",
    "                device, computeErrorLog, strategy=\"SIM\", sf=\"\"):\n",
    "#     torch.manual_seed(42)\n",
    "#     np.random.seed(42)\n",
    "    print(strategy, sf)\n",
    "    #load the dataset based on type of feature\n",
    "    train_set, val_set, test_set, lake_set, sel_cls_idx, num_cls = load_dataset_custom(datadir, dataset_name, feature, split_cfg, False, True)\n",
    "    print(\"selected classes are: \", sel_cls_idx)\n",
    "\n",
    "    if(feature==\"ood\"): num_cls+=1 #Add one class for OOD class\n",
    "    N = len(train_set)\n",
    "    trn_batch_size = 20\n",
    "    val_batch_size = 10\n",
    "    tst_batch_size = 100\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size,\n",
    "                                              shuffle=True, pin_memory=True)\n",
    "\n",
    "    valloader = torch.utils.data.DataLoader(val_set, batch_size=val_batch_size, \n",
    "                                            shuffle=False, pin_memory=True)\n",
    "\n",
    "    tstloader = torch.utils.data.DataLoader(test_set, batch_size=tst_batch_size,\n",
    "                                             shuffle=False, pin_memory=True)\n",
    "    \n",
    "    lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size,\n",
    "                                         shuffle=False, pin_memory=True)\n",
    "    true_lake_set = copy.deepcopy(lake_set)\n",
    "    # Budget for subset selection\n",
    "    bud = budget\n",
    "   \n",
    "    # Variables to store accuracies\n",
    "    fulltrn_losses = np.zeros(num_epochs)\n",
    "    val_losses = np.zeros(num_epochs)\n",
    "    tst_losses = np.zeros(num_epochs)\n",
    "    timing = np.zeros(num_epochs)\n",
    "    val_acc = np.zeros(num_epochs)\n",
    "    full_trn_acc = np.zeros(num_epochs)\n",
    "    tst_acc = np.zeros(num_epochs)\n",
    "    final_tst_predictions = []\n",
    "    final_tst_classifications = []\n",
    "    best_val_acc = -1\n",
    "    csvlog = []\n",
    "    val_csvlog = []\n",
    "    # Results logging file\n",
    "    print_every = 3\n",
    "#     all_logs_dir = '/content/drive/MyDrive/research/tdss/SMI_active_learning_results_woVal/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    all_logs_dir = './SMI_active_learning_results/' + dataset_name  + '/' + feature + '/'+  sf + '/' + str(bud) + '/' + str(run)\n",
    "    print(\"Saving results to: \", all_logs_dir)\n",
    "    subprocess.run([\"mkdir\", \"-p\", all_logs_dir])\n",
    "    exp_name = dataset_name + \"_\" + feature +  \"_\" + strategy + \"_\" + str(len(sel_cls_idx))  +\"_\" + sf +  '_budget:' + str(bud) + '_epochs:' + str(num_epochs) + '_linear:'  + str(linearLayer) + '_runs' + str(run)\n",
    "    print(exp_name)\n",
    "    res_dict = {\"dataset\":data_name, \n",
    "                \"feature\":feature, \n",
    "                \"sel_func\":sf,\n",
    "                \"sel_budget\":budget, \n",
    "                \"num_selections\":num_epochs, \n",
    "                \"model\":model_name, \n",
    "                \"learning_rate\":learning_rate, \n",
    "                \"setting\":split_cfg, \n",
    "                \"all_class_acc\":None, \n",
    "                \"test_acc\":[],\n",
    "                \"sel_per_cls\":[], \n",
    "                \"sel_cls_idx\":sel_cls_idx.tolist()}\n",
    "    # Model Creation\n",
    "    model = create_model(model_name, num_cls, device, embedding_type)\n",
    "    model1 = create_model(model_name, num_cls, device, embedding_type)\n",
    "    \n",
    "    # Loss Functions\n",
    "    criterion, criterion_nored = loss_function()\n",
    "    \n",
    "    strategy_args = {'batch_size': 20, 'device':'cuda', 'num_partitions':1, 'wrapped_strategy_class': None, \n",
    "         'embedding_type':'gradients', 'keep_embedding':False}\n",
    "    unlabeled_lake_set = LabeledToUnlabeledDataset(lake_set)\n",
    "    if(strategy == \"AL\"):\n",
    "        if(sf==\"badge\"):\n",
    "            strategy_sel = BADGE(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"us\"):\n",
    "            strategy_sel = EntropySampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"glister\" or sf==\"glister-tss\"):\n",
    "            strategy_sel = GLISTER(train_set, unlabeled_lake_set, model, num_cls, strategy_args, val_set, typeOf='rand', lam=0.1)\n",
    "        elif(sf==\"gradmatch-tss\"):\n",
    "            strategy_sel = GradMatchActive(train_set, unlabeled_lake_set, model, num_cls, strategy_args, val_set)\n",
    "        elif(sf==\"coreset\"):\n",
    "            strategy_sel = CoreSet(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"leastconf\"):\n",
    "            strategy_sel = LeastConfidence(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "        elif(sf==\"margin\"):\n",
    "            strategy_sel = MarginSampling(train_set, unlabeled_lake_set, model, num_cls, strategy_args)\n",
    "    if(strategy == \"SIM\"):\n",
    "        if(sf.endswith(\"mic\")):\n",
    "            strategy_args['scmi_function'] = sf.split(\"mic\")[0] + \"cmi\"\n",
    "            strategy_sel = SCMI(train_set, unlabeled_lake_set, val_set, val_set, model, num_cls, strategy_args)\n",
    "        if(sf.endswith(\"mi\")):\n",
    "            strategy_args['smi_function'] = sf\n",
    "            strategy_sel = SMI(train_set, unlabeled_lake_set, val_set, model, num_cls, strategy_args)\n",
    "        strategy_args['verbose'] = True\n",
    "        strategy_args['optimizer'] = \"LazyGreedy\"\n",
    "\n",
    "    # Getting the optimizer and scheduler\n",
    "#     optimizer, scheduler = optimizer_with_scheduler(model, num_epochs, learning_rate)\n",
    "    optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "    private_set = []\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        print(\"AL epoch: \", i)\n",
    "        tst_loss = 0\n",
    "        tst_correct = 0\n",
    "        tst_total = 0\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        if(i==0):\n",
    "            print(\"initial training epoch\")\n",
    "            if(os.path.exists(initModelPath)):\n",
    "                model.load_state_dict(torch.load(initModelPath, map_location=device))\n",
    "                print(\"Init model loaded from disk, skipping init training: \", initModelPath)\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    final_val_predictions = []\n",
    "                    final_val_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        val_loss += loss.item()\n",
    "                        if(feature==\"ood\"): \n",
    "                            _, predicted = outputs[...,:-1].max(1)\n",
    "                        else:\n",
    "                            _, predicted = outputs.max(1)\n",
    "                        val_total += targets.size(0)\n",
    "                        val_correct += predicted.eq(targets).sum().item()\n",
    "                        final_val_predictions += list(predicted.cpu().numpy())\n",
    "                        final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "  \n",
    "                    final_tst_predictions = []\n",
    "                    final_tst_classifications = []\n",
    "                    for batch_idx, (inputs, targets) in enumerate(tstloader):\n",
    "                        inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        tst_loss += loss.item()\n",
    "                        if(feature==\"ood\"): \n",
    "                            _, predicted = outputs[...,:-1].max(1)\n",
    "                        else:\n",
    "                            _, predicted = outputs.max(1)\n",
    "                        tst_total += targets.size(0)\n",
    "                        tst_correct += predicted.eq(targets).sum().item()\n",
    "                        final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                        final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "                    best_val_acc = (val_correct/val_total)\n",
    "                    val_acc[i] = val_correct / val_total\n",
    "                    tst_acc[i] = tst_correct / tst_total\n",
    "                    val_losses[i] = val_loss\n",
    "                    tst_losses[i] = tst_loss\n",
    "                    res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "                continue\n",
    "        else:\n",
    "            unlabeled_lake_set = LabeledToUnlabeledDataset(lake_set)\n",
    "            strategy_sel.update_data(train_set, unlabeled_lake_set)\n",
    "            #compute the error log before every selection\n",
    "            if(computeErrorLog):\n",
    "                tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "                csvlog.append(tst_err_log)\n",
    "                val_csvlog.append(val_err_log)\n",
    "            ####SIM####\n",
    "            if(strategy==\"SIM\" or strategy==\"SF\"):\n",
    "                if(sf.endswith(\"mi\")):\n",
    "                    if(feature==\"classimb\"):\n",
    "                        #make a dataloader for the misclassifications - only for experiments with targets\n",
    "                        miscls_set = getQuerySet(val_set, val_class_err_idxs, sel_cls_idx, miscls)\n",
    "                        strategy_sel.update_queries(miscls_set)\n",
    "                elif(sf.endswith(\"mic\")): #configured for the OOD setting\n",
    "                    print(\"val set targets: \", val_set.targets)\n",
    "                    strategy_sel.update_queries(val_set) #In-dist samples are in Val \n",
    "                    if(len(private_set)!=0):\n",
    "                        print(\"private set targets: \", private_set.targets)\n",
    "                        strategy_sel.update_privates(private_set)\n",
    "\n",
    "            ###AL###\n",
    "            elif(strategy==\"AL\"):\n",
    "                if(sf==\"glister-tss\" or sf==\"gradmatch-tss\"):\n",
    "                    miscls_set = getQuerySet(val_set, val_class_err_idxs, sel_cls_idx, miscls)\n",
    "                    strategy_sel.update_queries(miscls_set)\n",
    "                    print(\"reinit AL with targeted miscls samples\")\n",
    "                \n",
    "            elif(strategy==\"random\"):\n",
    "                subset = np.random.choice(np.array(list(range(len(lake_set)))), size=budget, replace=False)\n",
    "            \n",
    "            strategy_sel.update_model(model)\n",
    "            subset = strategy_sel.select(budget)\n",
    "#             print(\"True targets of subset: \", torch.Tensor(true_lake_set.targets.float())[subset])\n",
    "#             hypothesized_targets = strategy_sel.predict(unlabeled_lake_set)\n",
    "#             print(\"Hypothesized targets of subset: \", hypothesized_targets)\n",
    "            print(\"#### SELECTION COMPLETE ####\")\n",
    "            lake_subset_idxs = subset #indices wrt to lake that need to be removed from the lake\n",
    "            if(feature==\"ood\"): #remove ood points from the subset\n",
    "                subset = remove_ood_points(true_lake_set, subset, sel_cls_idx)\n",
    "            \n",
    "            print(\"selEpoch: %d, Selection Ended at:\" % (i), str(datetime.datetime.now()))\n",
    "            perClsSel = getPerClassSel(true_lake_set, lake_subset_idxs, num_cls)\n",
    "            res_dict['sel_per_cls'].append(perClsSel)\n",
    "            \n",
    "            #augment the train_set with selected indices from the lake\n",
    "            if(feature==\"classimb\"):\n",
    "                train_set, lake_set, true_lake_set, add_val_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget, True) #aug train with random if budget is not filled\n",
    "                if(augTarget): val_set = ConcatWithTargets(val_set, add_val_set)\n",
    "            elif(feature==\"ood\"):\n",
    "                train_set, lake_set, true_lake_set, new_private_set, add_val_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget)\n",
    "                train_set = torch.utils.data.ConcatDataset([train_set, new_private_set]) #Add the OOD samples with a common OOD class\n",
    "                val_set = ConcatWithTargets(val_set, add_val_set)\n",
    "                if(len(private_set)!=0):\n",
    "                    private_set = ConcatWithTargets(private_set, new_private_set)\n",
    "                else:\n",
    "                    private_set = new_private_set\n",
    "            else:\n",
    "                train_set, lake_set, true_lake_set = aug_train_subset(train_set, lake_set, true_lake_set, subset, lake_subset_idxs, budget)\n",
    "            print(\"After augmentation, size of train_set: \", len(train_set), \" lake set: \", len(lake_set), \" val set: \", len(val_set))\n",
    "    \n",
    "#           Reinit train and lake loaders with new splits and reinit the model\n",
    "            trainloader = torch.utils.data.DataLoader(train_set, batch_size=trn_batch_size, shuffle=True, pin_memory=True)\n",
    "            lakeloader = torch.utils.data.DataLoader(lake_set, batch_size=tst_batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "            if(augTarget):\n",
    "              valloader = torch.utils.data.DataLoader(val_set, batch_size=len(val_set), shuffle=False, pin_memory=True)\n",
    "            model = create_model(model_name, num_cls, device, strategy_args['embedding_type'])\n",
    "            optimizer = optimizer_without_scheduler(model, learning_rate)\n",
    "                \n",
    "        #Start training\n",
    "        start_time = time.time()\n",
    "        num_ep=1\n",
    "        while(full_trn_acc[i]<0.99 and num_ep<300):\n",
    "            model.train()\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                # Variables in Pytorch are differentiable.\n",
    "                inputs, target = Variable(inputs), Variable(inputs)\n",
    "                # This will zero out the gradients for this batch.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#             scheduler.step()\n",
    "          \n",
    "            full_trn_loss = 0\n",
    "            full_trn_correct = 0\n",
    "            full_trn_total = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    full_trn_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    full_trn_total += targets.size(0)\n",
    "                    full_trn_correct += predicted.eq(targets).sum().item()\n",
    "                full_trn_acc[i] = full_trn_correct / full_trn_total\n",
    "                print(\"Selection Epoch \", i, \" Training epoch [\" , num_ep, \"]\" , \" Training Acc: \", full_trn_acc[i], end=\"\\r\")\n",
    "                num_ep+=1\n",
    "            timing[i] = time.time() - start_time\n",
    "        with torch.no_grad():\n",
    "            final_val_predictions = []\n",
    "            final_val_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(valloader): #Compute Val accuracy\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                if(feature==\"ood\"): \n",
    "                    _, predicted = outputs[...,:-1].max(1)\n",
    "                else:\n",
    "                    _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                final_val_predictions += list(predicted.cpu().numpy())\n",
    "                final_val_classifications += list(predicted.eq(targets).cpu().numpy())\n",
    "\n",
    "            final_tst_predictions = []\n",
    "            final_tst_classifications = []\n",
    "            for batch_idx, (inputs, targets) in enumerate(tstloader): #Compute test accuracy\n",
    "                inputs, targets = inputs.to(device), targets.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                tst_loss += loss.item()\n",
    "                if(feature==\"ood\"): \n",
    "                    _, predicted = outputs[...,:-1].max(1)\n",
    "                else:\n",
    "                    _, predicted = outputs.max(1)\n",
    "                tst_total += targets.size(0)\n",
    "                tst_correct += predicted.eq(targets).sum().item()\n",
    "                final_tst_predictions += list(predicted.cpu().numpy())\n",
    "                final_tst_classifications += list(predicted.eq(targets).cpu().numpy())                \n",
    "            val_acc[i] = val_correct / val_total\n",
    "            tst_acc[i] = tst_correct / tst_total\n",
    "            val_losses[i] = val_loss\n",
    "            fulltrn_losses[i] = full_trn_loss\n",
    "            tst_losses[i] = tst_loss\n",
    "            full_val_acc = list(np.array(val_acc))\n",
    "            full_timing = list(np.array(timing))\n",
    "            res_dict[\"test_acc\"].append(tst_acc[i])\n",
    "            print('Epoch:', i + 1, 'FullTrn,TrainAcc,ValLoss,ValAcc,TstLoss,TstAcc,Time:', full_trn_loss, full_trn_acc[i], val_loss, val_acc[i], tst_loss, tst_acc[i], timing[i])\n",
    "        if(i==0): \n",
    "            print(\"saving initial model\") \n",
    "            torch.save(model.state_dict(), initModelPath) #save initial train model if not present\n",
    "    if(computeErrorLog):\n",
    "        tst_err_log, val_err_log, val_class_err_idxs = find_err_per_class(test_set, val_set, final_val_classifications, final_val_predictions, final_tst_classifications, final_tst_predictions, all_logs_dir, sf+\"_\"+str(bud))\n",
    "        csvlog.append(tst_err_log)\n",
    "        val_csvlog.append(val_err_log)\n",
    "        print(csvlog)\n",
    "        res_dict[\"all_class_acc\"] = csvlog\n",
    "        res_dict[\"all_val_class_acc\"] = val_csvlog\n",
    "        with open(os.path.join(all_logs_dir, exp_name+\".csv\"), \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(csvlog)\n",
    "    #save results dir with test acc and per class selections\n",
    "    with open(os.path.join(all_logs_dir, exp_name+\".json\"), 'w') as fp:\n",
    "        json.dump(res_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp9_ZU7I8lLa"
   },
   "source": [
    "# FLCMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZKsblhs8lLa",
    "outputId": "5983fdfb-6541-43e9-a0f0-304ddfa543b6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'flmic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGDETCMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'logdetmic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FL2MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'fl2mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeujlNrf8lLa"
   },
   "source": [
    "# FL1MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491,
     "referenced_widgets": [
      "d978b42c8c2d4ce69ca7a3233094ba73",
      "14a33535616d4fa993d20e525666af04",
      "bb6c0909711948b98f0a0b767cb9201e",
      "6792820b58df4d2b86386c29aaad1a9f",
      "9d386f74f6c4469bbbf1ac6b2a49afed",
      "4ee6a0573c6845458996a59679ac6068",
      "0074c4d4c28d4dad87a3eaaac8458479",
      "cbe081bb90d04faaae98fef1c9921180"
     ]
    },
    "id": "QHn0QzY78lLa",
    "outputId": "a203ea99-fb5b-4bfa-99e5-699ef0715d16",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'fl1mi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ou4D7n48lLb"
   },
   "source": [
    "# BADGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9_mFqNi8lLb",
    "outputId": "f1793df9-ba1f-4b87-c89a-d3ad7135f4f7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"badge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kwjxMFh8lLb"
   },
   "source": [
    "# US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CF4un-LA8lLb",
    "outputId": "c5160f72-26ec-4601-e508-50235d4d9a70",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPHhNlC58lLb"
   },
   "source": [
    "# GLISTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZKQg16xY8lLb",
    "outputId": "ce3798dc-6b3d-4194-a9a2-91ee67d63bdb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"glister-tss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZmWi33K8lLc"
   },
   "source": [
    "# GCMI+DIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orRudR4s8lLc",
    "outputId": "9084e7ee-0dd4-41e0-f2c3-37b3658da3c0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'div-gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9f6qyrpA8lLc"
   },
   "source": [
    "# GCMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MM9copHy8lLc",
    "outputId": "1c5e4fe8-2f3c-4137-9f64-c728e00c34d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'gcmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGDPAjDR8lLc"
   },
   "source": [
    "# LOGDETMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1QbcrJk8lLd",
    "outputId": "1a7604b8-dee7-468f-d9bf-7e8b202c18d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SIM\",'logdetmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9alLsFyi8lLd"
   },
   "source": [
    "# FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qg1ksWWJ8lLd",
    "outputId": "30627285-7fde-482f-c247-d6cecf305334",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SF\",'fl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQdih1_X8lLd"
   },
   "source": [
    "# LOGDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "wy8Qn5z-8lLd",
    "outputId": "a0d538d1-4ccf-4879-d4d2-ab86fcc95114",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"SF\",'logdet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3dZqCZ1v8lLe"
   },
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlbOpp438lLe",
    "outputId": "f754727a-dfb5-4853-d4c1-8a96115182db",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"random\",'random')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4sDppX8vFnw"
   },
   "source": [
    "# CORESET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MgeWSB4qvJmJ",
    "outputId": "10e62461-d81c-4974-bae7-0526c7c8f5b5"
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"coreset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zPJubq7vVwc"
   },
   "source": [
    "# LEASTCONF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmVWElmyvUED",
    "outputId": "a56dce6a-8fed-4684-b2f5-8e538b94f93d"
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"leastconf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pkk2sbuvdak"
   },
   "source": [
    "# MARGIN SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxIT27hqvcI3",
    "outputId": "8dcef4b6-a336-4bb0-ec4d-d8108c22f6bf"
   },
   "outputs": [],
   "source": [
    "train_model_al(datkbuildPath, exePath, num_epochs, data_name, datadir, feature, model_name, budget, split_cfg, learning_rate, run, device, computeClassErrorLog, \"AL\",\"margin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "submodlib_cifar_classimb.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0074c4d4c28d4dad87a3eaaac8458479": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14a33535616d4fa993d20e525666af04": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ee6a0573c6845458996a59679ac6068": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6792820b58df4d2b86386c29aaad1a9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbe081bb90d04faaae98fef1c9921180",
      "placeholder": "​",
      "style": "IPY_MODEL_0074c4d4c28d4dad87a3eaaac8458479",
      "value": " 170499072/? [00:07&lt;00:00, 23215900.76it/s]"
     }
    },
    "9d386f74f6c4469bbbf1ac6b2a49afed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bb6c0909711948b98f0a0b767cb9201e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ee6a0573c6845458996a59679ac6068",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d386f74f6c4469bbbf1ac6b2a49afed",
      "value": 170498071
     }
    },
    "cbe081bb90d04faaae98fef1c9921180": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d978b42c8c2d4ce69ca7a3233094ba73": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb6c0909711948b98f0a0b767cb9201e",
       "IPY_MODEL_6792820b58df4d2b86386c29aaad1a9f"
      ],
      "layout": "IPY_MODEL_14a33535616d4fa993d20e525666af04"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

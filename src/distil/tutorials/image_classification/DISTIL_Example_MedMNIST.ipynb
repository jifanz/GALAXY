{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DISTIL_Example_MedMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8AxA7KVfv9m"
      },
      "source": [
        "# **DISTIL Usage Example: MedMNIST**\n",
        "\n",
        "Here, we show how to use DISTIL to perform active learning on image classification tasks (MedMNIST's OrganAMNIST). This notebook can be easily executed on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Y2r9Y_fH5B"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bsVDupO_EOh"
      },
      "source": [
        "# Get DISTIL\n",
        "!git clone https://github.com/decile-team/distil.git\n",
        "!pip install -r distil/requirements/requirements.txt\n",
        "\n",
        "# Get MedMNIST\n",
        "!git clone https://github.com/MedMNIST/MedMNIST.git\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sys.path.append(\"MedMNIST/\")\n",
        "from medmnist import OrganAMNIST\n",
        "\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import cifar\n",
        "\n",
        "sys.path.append('distil/')\n",
        "from distil.active_learning_strategies import GLISTER, BADGE, EntropySampling, RandomSampling   # All active learning strategies showcased in this example\n",
        "from distil.utils.models.resnet import ResNet18                                                 # The model used in our image classification example\n",
        "from distil.utils.train_helper import data_train                                                # A utility training class provided by DISTIL\n",
        "from distil.utils.utils import LabeledToUnlabeledDataset                                        # A utility wrapper class that removes labels from labeled PyTorch dataset objects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvn_Dal4gb8S"
      },
      "source": [
        "## Preparing OrganAMNIST\n",
        "\n",
        "The MedMNIST dataset OrganAMNIST contains 58,850 28x28 monochrome images (34,581 train, 6,491 validation, 17,778 test) in 11 different classes. The 11 different classes represent 11 different organs, and the task is to correctly classify a given axial view of an organ. Here, we do a simple setup of the OrganAMNIST dataset that we will use in this example. More importantly, we define a split on OrganAMNIST's training set into an initial labeled seed set and an unlabeled set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J39lJ2uWVpbM"
      },
      "source": [
        "**Calculate Average/STD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khfi12OlSTSz"
      },
      "source": [
        "# We do not have the average and standard deviation to use for data normalization. Here, we compute \n",
        "# it on a per-pixel basis, and we use the training set only for this calculation.\n",
        "\n",
        "train_dataset = OrganAMNIST(root=\".\", split=\"train\", download=True, transform=transforms.ToTensor())\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "average = 0.\n",
        "num_pixels = 0\n",
        "for batch_idx, (image, label) in enumerate(train_dataloader):\n",
        "    image = image.to(\"cuda\")\n",
        "    average += image.sum()\n",
        "    num_pixels += len(image.flatten())\n",
        "average = average.item() / num_pixels\n",
        "\n",
        "var = 0.\n",
        "for batch_idx, (image, label) in enumerate(train_dataloader):\n",
        "    image = image.to(\"cuda\").flatten()\n",
        "    image_diff = image - average\n",
        "    image_var = torch.dot(image_diff, image_diff)\n",
        "    var += image_var\n",
        "\n",
        "std = math.sqrt(var / (num_pixels - 1))\n",
        "\n",
        "print(\"AVERAGE:\",average)\n",
        "print(\"STD:\",std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm-wkPNJVwo-"
      },
      "source": [
        "**Create OrganAMNIST Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdLy9wyjCuT-"
      },
      "source": [
        "data_set_name = 'OrganAMNIST'\n",
        "download_path = '.'\n",
        "\n",
        "# Define transforms on the dataset splits of OrganAMNIST. Here, we use random crops and horizontal flips for training augmentations.\n",
        "# Both the train and test sets are converted to PyTorch tensors and are normalized around the mean/std of OrganAMNIST.\n",
        "organ_amnist_training_transform = transforms.Compose([transforms.RandomCrop(28, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "organ_amnist_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "\n",
        "# OrganAMNIST provides its labels in list form; so we need to get the original label\n",
        "target_transform = lambda x: x[0]\n",
        "\n",
        "# Get the dataset objects from MedMNIST. Here, OrganAMNIST is downloaded, and the transform is applied when points \n",
        "# are retrieved.\n",
        "organ_amnist_full_train = OrganAMNIST(root=download_path, split=\"train\", download=True, transform=organ_amnist_training_transform, target_transform=target_transform)\n",
        "organ_amnist_test = OrganAMNIST(root=download_path, split=\"test\", download=True, transform=organ_amnist_test_transform, target_transform=target_transform)\n",
        "\n",
        "# Get the dimension of the images. Here, we simply take the very first image of OrganAMNIST\n",
        "# and query its dimension.\n",
        "dim = np.shape(organ_amnist_full_train[0][0])\n",
        "\n",
        "# We now define a train-unlabeled split for the sake of the experiment. Here, we simply take 1000 points as the initial seed set.\n",
        "# The rest of the points are taken as the unlabeled set. While the unlabeled set constructed here technically has labels, they \n",
        "# are only used when querying for labels. Hence, they only exist here for the sake of experimental design.\n",
        "train_size = 250\n",
        "organ_amnist_train = Subset(organ_amnist_full_train, list(range(train_size)))\n",
        "organ_amnist_unlabeled = Subset(organ_amnist_full_train, list(range(train_size, len(organ_amnist_full_train))))\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in OrganAMNIST\n",
        "nclasses = 11\n",
        "n_rounds = 7\n",
        "budget = 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVwJ7RcOhVc7"
      },
      "source": [
        "## Preparing the Model\n",
        "\n",
        "Here, we use DISTIL's provided implementation of the [ResNet-18](https://arxiv.org/abs/1512.03385) architecture. We also create a model directory to store trained models in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pHzwNwvhVts"
      },
      "source": [
        "net = ResNet18(num_classes=nclasses, channels=1)\n",
        "base_dir = \"models\"\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spOTFeWgfm4b"
      },
      "source": [
        "## Training an Initial Model\n",
        "Here, we train an initial model. We do so by creating a training loop object on the initial seed set, the model architecture, and a list of provided arguments. We then save the initial model. Note: If you've already run the first cell, then you can simply run the second cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru5tj6u8fwEi"
      },
      "source": [
        "# Specify additional training parameters. Here, we set the maximum number of epochs of training to 300, \n",
        "# the learning rate to 0.01, the batch size to 20, the maximum train accuracy of training to 0.99, and \n",
        "# the optimizer to stochastic gradient descent.\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(organ_amnist_train, net, args)\n",
        "clf = dt.train()\n",
        "torch.save(clf.state_dict(), model_directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd-ieb7RgyJY"
      },
      "source": [
        "base_dir = \"models\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhsUx57BjBT2"
      },
      "source": [
        "## Active Learning Strategies\n",
        "\n",
        "Here, we show examples of a couple active learning strategies being used in the setting of image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Bxn6LmCeEI"
      },
      "source": [
        "### Random Sampling\n",
        "This strategy is often used as a baseline, where we pick a subset of unlabeled points randomly. Here we create a instance of distil.active_learning_strategies.random_sampling.RandomSampling by passing following parameters:\n",
        "\n",
        "**training_dataset** – The labeled dataset\n",
        "\n",
        "**unlabeled_dataset** – The unlabeled dataset, which has a wrapper around it that strips the label\n",
        "\n",
        "**net (class object)** – Model architecture used for training. Could be instance of models defined in distil.utils.models or something similar.\n",
        "\n",
        "**nclasses (int)** – No. of classes in tha dataset\n",
        "\n",
        "**args (dictionary)**– This dictionary should have ‘batch_size’ as a key. 'batch_size' should be such that one can exploit the benefits of tensorization while honouring the resourse constraits. This ‘batch_size’ therefore can be different than the one used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8OdeQ63DZuP"
      },
      "source": [
        "# Initialize the random sampling AL strategy. Note: The labels are shaved off the unlabeled dataset above to match the setting.\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = RandomSampling(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(organ_amnist_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(organ_amnist_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    organ_amnist_full_train.transform = organ_amnist_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    organ_amnist_full_train.transform = organ_amnist_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    organ_amnist_train = ConcatDataset([organ_amnist_train, Subset(organ_amnist_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(organ_amnist_unlabeled))) - set(idx))\n",
        "    organ_amnist_unlabeled = Subset(organ_amnist_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(organ_amnist_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled))\n",
        "    dt.update_data(organ_amnist_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(organ_amnist_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "# Lastly, we save the accuracies in case a comparison is warranted.\n",
        "with open(os.path.join(base_dir,'random.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm_ILTinrkLk"
      },
      "source": [
        "### Entropy Sampling\n",
        "A very basic strategy to select unlabeled points is entropy sampling, where we select samples about which the model is most uncertain by measuring the entropy of the class prediction. Hence, a valid strategy is to select those points in the unlabeled set with highest entropy (maximum uncertainty). Specifically, let $z_i$ be output from the model. By applying a softmax, we obtain probabilities that we can use: $$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$ Then, the entropy can be calculated as $$ENTROPY = -\\sum_j \\sigma(z)_j*log(\\sigma(z)_j)$$\n",
        "\n",
        "Here we create a instance of distil.active_learning_strategies.entropy_sampling.EntropySampling with the same parameters passed to distil.active_learning_strategies.random_sampling.RandomSampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieS3rs-TC8bE"
      },
      "source": [
        "**Reloading Base Model & Data**\n",
        "\n",
        "We make sure the fixture is the same by repeating the same setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwowelOwZRGJ"
      },
      "source": [
        "data_set_name = 'OrganAMNIST'\n",
        "download_path = '.'\n",
        "\n",
        "# Define transforms on the dataset splits of OrganAMNIST. Here, we use random crops and horizontal flips for training augmentations.\n",
        "# Both the train and test sets are converted to PyTorch tensors and are normalized around the mean/std of OrganAMNIST.\n",
        "organ_amnist_training_transform = transforms.Compose([transforms.RandomCrop(28, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "organ_amnist_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "\n",
        "# OrganAMNIST provides its labels in list form; so we need to get the original label\n",
        "target_transform = lambda x: x[0]\n",
        "\n",
        "# Get the dataset objects from MedMNIST. Here, OrganAMNIST is downloaded, and the transform is applied when points \n",
        "# are retrieved.\n",
        "organ_amnist_full_train = OrganAMNIST(root=download_path, split=\"train\", download=True, transform=organ_amnist_training_transform, target_transform=target_transform)\n",
        "organ_amnist_test = OrganAMNIST(root=download_path, split=\"test\", download=True, transform=organ_amnist_test_transform, target_transform=target_transform)\n",
        "\n",
        "# Get the dimension of the images. Here, we simply take the very first image of OrganAMNIST\n",
        "# and query its dimension.\n",
        "dim = np.shape(organ_amnist_full_train[0][0])\n",
        "\n",
        "# We now define a train-unlabeled split for the sake of the experiment. Here, we simply take 1000 points as the initial seed set.\n",
        "# The rest of the points are taken as the unlabeled set. While the unlabeled set constructed here technically has labels, they \n",
        "# are only used when querying for labels. Hence, they only exist here for the sake of experimental design.\n",
        "train_size = 250\n",
        "organ_amnist_train = Subset(organ_amnist_full_train, list(range(train_size)))\n",
        "organ_amnist_unlabeled = Subset(organ_amnist_full_train, list(range(train_size, len(organ_amnist_full_train))))\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in OrganAMNIST\n",
        "nclasses = 11\n",
        "n_rounds = 7\n",
        "budget = 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcx_jnorZRGW"
      },
      "source": [
        "net = ResNet18(num_classes=nclasses, channels=1)\n",
        "base_dir = \"models\"\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfMZmggtZRGX"
      },
      "source": [
        "base_dir = \"models\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3C0729a2Vzg"
      },
      "source": [
        "# Initialize the random sampling AL strategy. Note: The labels are shaved off the unlabeled dataset above to match the setting.\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = EntropySampling(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(organ_amnist_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(organ_amnist_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    organ_amnist_full_train.transform = organ_amnist_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    organ_amnist_full_train.transform = organ_amnist_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    organ_amnist_train = ConcatDataset([organ_amnist_train, Subset(organ_amnist_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(organ_amnist_unlabeled))) - set(idx))\n",
        "    organ_amnist_unlabeled = Subset(organ_amnist_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(organ_amnist_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled))\n",
        "    dt.update_data(organ_amnist_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(organ_amnist_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "# Lastly, we save the accuracies in case a comparison is warranted.\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGqAV0GrwwN"
      },
      "source": [
        "### BADGE\n",
        "This method is based on the paper [Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds](https://arxiv.org/abs/1906.03671). The strategy is meant to select points that are both diverse (e.g., their embeddings span multiple directions) and uncertain (e.g., their contribution to the loss is large). The following steps are taken:\n",
        "\n",
        "* Calculate the pseudo-label for each point in the unlabeled set. The pseudo-label is the class with the highest probability.\n",
        "* Compute the cross-entropy loss for each point in the unlabeled set using this pseudo-label.\n",
        "* Obtain the resulting loss gradients on the last linear layer of the model for each point. (These are referred to as the hypothesized loss gradients.)\n",
        "* Using these gradients as a form of embedding for each unlabeled point, run k-means++ initialization on this embedding set, retrieving $k$ centers. Each center is a point from the unlabeled set, and $k$ represents the active learning budget.\n",
        "* Request labels for the $k$ points whose embeddings were selected.\n",
        "\n",
        "Here we create a instance of distil.active_learning_strategies.badge.BADGE with same parameters passed to distil.active_learning_strategies.random_sampling.RandomSampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z2N1gsCGE8U"
      },
      "source": [
        "**Reloading Base Model & Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7a506L-ZqK6"
      },
      "source": [
        "data_set_name = 'OrganAMNIST'\n",
        "download_path = '.'\n",
        "\n",
        "# Define transforms on the dataset splits of OrganAMNIST. Here, we use random crops and horizontal flips for training augmentations.\n",
        "# Both the train and test sets are converted to PyTorch tensors and are normalized around the mean/std of OrganAMNIST.\n",
        "organ_amnist_training_transform = transforms.Compose([transforms.RandomCrop(28, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "organ_amnist_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((average,), (std,))])\n",
        "\n",
        "# OrganAMNIST provides its labels in list form; so we need to get the original label\n",
        "target_transform = lambda x: x[0]\n",
        "\n",
        "# Get the dataset objects from MedMNIST. Here, OrganAMNIST is downloaded, and the transform is applied when points \n",
        "# are retrieved.\n",
        "organ_amnist_full_train = OrganAMNIST(root=download_path, split=\"train\", download=True, transform=organ_amnist_training_transform, target_transform=target_transform)\n",
        "organ_amnist_test = OrganAMNIST(root=download_path, split=\"test\", download=True, transform=organ_amnist_test_transform, target_transform=target_transform)\n",
        "\n",
        "# Get the dimension of the images. Here, we simply take the very first image of OrganAMNIST\n",
        "# and query its dimension.\n",
        "dim = np.shape(organ_amnist_full_train[0][0])\n",
        "\n",
        "# We now define a train-unlabeled split for the sake of the experiment. Here, we simply take 1000 points as the initial seed set.\n",
        "# The rest of the points are taken as the unlabeled set. While the unlabeled set constructed here technically has labels, they \n",
        "# are only used when querying for labels. Hence, they only exist here for the sake of experimental design.\n",
        "train_size = 250\n",
        "organ_amnist_train = Subset(organ_amnist_full_train, list(range(train_size)))\n",
        "organ_amnist_unlabeled = Subset(organ_amnist_full_train, list(range(train_size, len(organ_amnist_full_train))))\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in OrganAMNIST\n",
        "nclasses = 11\n",
        "n_rounds = 7\n",
        "budget = 250"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmENx4rgZqLB"
      },
      "source": [
        "net = ResNet18(num_classes=nclasses, channels=1)\n",
        "base_dir = \"models\"\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OE6gmSgxZqLB"
      },
      "source": [
        "base_dir = \"models\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF4-oHDPZqLC"
      },
      "source": [
        "# Initialize the random sampling AL strategy. Note: The labels are shaved off the unlabeled dataset above to match the setting.\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = BADGE(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(organ_amnist_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(organ_amnist_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    organ_amnist_full_train.transform = organ_amnist_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    organ_amnist_full_train.transform = organ_amnist_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    organ_amnist_train = ConcatDataset([organ_amnist_train, Subset(organ_amnist_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(organ_amnist_unlabeled))) - set(idx))\n",
        "    organ_amnist_unlabeled = Subset(organ_amnist_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(organ_amnist_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(organ_amnist_train, LabeledToUnlabeledDataset(organ_amnist_unlabeled))\n",
        "    dt.update_data(organ_amnist_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(organ_amnist_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "# Lastly, we save the accuracies in case a comparison is warranted.\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNZjPPO7java"
      },
      "source": [
        "## Visualizing the Results\n",
        "\n",
        "If all strategies have run to completion, you can run the following cell to view the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vn65GnhZosp"
      },
      "source": [
        "# Load the accuracies previously obtained\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_en = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_bd = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'random.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_rd = [round(float(x)*100, 2) for x in acc_]\n",
        "\n",
        "# Plot them using matplotlib\n",
        "x_axis = np.array([train_size+budget*i for i in range(n_rounds)])\n",
        "plt.figure()\n",
        "plt.plot(x_axis, acc_en, 'g-', label='UNCERTAINTY',marker='o')\n",
        "plt.plot(x_axis, acc_bd, 'c', label='BADGE',marker='o')\n",
        "plt.plot(x_axis, acc_rd, 'r', label='RANDOM',marker='o')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('No of Images')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('DISTIL_OrganAMNIST')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DISTIL_Example_Redundancy_CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "32b2a50fc68443ccb3b68df30e759c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5765a9a83d6b4080b2efe023690cda58",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d9faa4ce87b5492caaf2f7bd0266c6bc",
              "IPY_MODEL_6db98e4e34df42608c906d36dba5dd0d",
              "IPY_MODEL_8e0e3722ddb346858e2eaf6e48e111d5"
            ]
          }
        },
        "5765a9a83d6b4080b2efe023690cda58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d9faa4ce87b5492caaf2f7bd0266c6bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f5065cbec814f3a8f64f00a539ddfd0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_08edb2d80422484c8958441915a601fe"
          }
        },
        "6db98e4e34df42608c906d36dba5dd0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_96f83d3a10874abd9b68f28ca4c48bc5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b7dd6f68bfa4b109a04e193fb2215d6"
          }
        },
        "8e0e3722ddb346858e2eaf6e48e111d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bb9f212b37784774995dbfbac41b0dba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:02&lt;00:00, 79740797.96it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2afbd98351bd49659a65b2a8d64c41b4"
          }
        },
        "7f5065cbec814f3a8f64f00a539ddfd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "08edb2d80422484c8958441915a601fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "96f83d3a10874abd9b68f28ca4c48bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b7dd6f68bfa4b109a04e193fb2215d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb9f212b37784774995dbfbac41b0dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2afbd98351bd49659a65b2a8d64c41b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOkAr6RnQ2BJ"
      },
      "source": [
        "# DISTIL Usage Example: CIFAR-10 with Redundancy\n",
        "\n",
        "Occasionally, a labeled dataset may not have enough data to yield good model performance. The easiest fix is to add more data by selecting and labeling unlabeled instances from a large pool of unlabeled data. However, the relative ease of acquiring unlabeled data in certain instances makes it very easy to accrue massive amounts of redundant examples. Adding redundant examples to a labeled dataset does not yield substantive performance benefits for the added computational cost. So, how do we select examples from the unlabeled pool that are not redundant with respect to our labeled dataset? Here, we show how to use DISTIL's implementation of [SIMILAR](https://arxiv.org/abs/2107.00717) to avoid selecting redundant data."
      ],
      "id": "lOkAr6RnQ2BJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ibaupa-l2T_"
      },
      "source": [
        "# Preparation"
      ],
      "id": "-Ibaupa-l2T_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViAJ7rawfp53"
      },
      "source": [
        "## Installation and Imports"
      ],
      "id": "ViAJ7rawfp53"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmhefeJQfqKb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a769bb9b-2291-4857-8827-709b15d8013c"
      },
      "source": [
        "# Get DISTIL\n",
        "!git clone https://github.com/decile-team/distil.git\n",
        "!pip install -r distil/requirements/requirements.txt\n",
        "\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import cifar\n",
        "\n",
        "sys.path.append('distil/')\n",
        "from distil.active_learning_strategies import SCG, GLISTER, BADGE, EntropySampling, RandomSampling  # All active learning strategies showcased in this example\n",
        "from distil.utils.models.resnet import ResNet18                                                     # The model used in our image classification example\n",
        "from distil.utils.train_helper import data_train                                                    # A utility training class provided by DISTIL\n",
        "from distil.utils.utils import LabeledToUnlabeledDataset                                            # A utility wrapper class that removes labels from labeled PyTorch dataset objects"
      ],
      "id": "nmhefeJQfqKb",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'distil'...\n",
            "remote: Enumerating objects: 3324, done.\u001b[K\n",
            "remote: Counting objects: 100% (1281/1281), done.\u001b[K\n",
            "remote: Compressing objects: 100% (812/812), done.\u001b[K\n",
            "remote: Total 3324 (delta 794), reused 841 (delta 461), pack-reused 2043\u001b[K\n",
            "Receiving objects: 100% (3324/3324), 23.05 MiB | 24.05 MiB/s, done.\n",
            "Resolving deltas: 100% (2067/2067), done.\n",
            "Looking in indexes: https://test.pypi.org/simple/, https://pypi.org/simple/\n",
            "Collecting sphinxcontrib-bibtex>=2.3.0\n",
            "  Downloading sphinxcontrib_bibtex-2.4.1-py3-none-any.whl (38 kB)\n",
            "Collecting multipledispatch==0.6.0\n",
            "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting scikit-learn==0.23.0\n",
            "  Downloading scikit_learn-0.23.0-cp37-cp37m-manylinux1_x86_64.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 7.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 6)) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm>=4.24.0 in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 7)) (4.62.3)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 8)) (0.51.2)\n",
            "Collecting submodlib\n",
            "  Downloading https://test-files.pythonhosted.org/packages/55/62/88e02a0e170498f38f7b9ce22b3e0a6a3cf9c82a33d3553da693c5c52872/submodlib-1.1.5.tar.gz (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.5 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 12)) (1.1.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r distil/requirements/requirements.txt (line 13)) (0.11.1+cu111)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from multipledispatch==0.6.0->-r distil/requirements/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.0->-r distil/requirements/requirements.txt (line 3)) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.0->-r distil/requirements/requirements.txt (line 3)) (1.1.0)\n",
            "Collecting Sphinx>=2.1\n",
            "  Downloading Sphinx-4.3.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 38.6 MB/s \n",
            "\u001b[?25hCollecting pybtex>=0.20\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[K     |████████████████████████████████| 561 kB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docutils>=0.8 in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (0.17.1)\n",
            "Collecting pybtex-docutils>=1.0.0\n",
            "  Downloading pybtex_docutils-1.0.1-py3-none-any.whl (4.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->-r distil/requirements/requirements.txt (line 6)) (3.10.0.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->-r distil/requirements/requirements.txt (line 8)) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->-r distil/requirements/requirements.txt (line 8)) (57.4.0)\n",
            "Collecting latexcodec>=1.0.4\n",
            "  Downloading latexcodec-2.0.1-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: PyYAML>=3.01 in /usr/local/lib/python3.7/dist-packages (from pybtex>=0.20->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (3.13)\n",
            "Collecting sphinxcontrib-applehelp\n",
            "  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[K     |████████████████████████████████| 121 kB 67.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (0.7.12)\n",
            "Collecting sphinxcontrib-jsmath\n",
            "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.11.3)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.9.1)\n",
            "Collecting sphinxcontrib-devhelp\n",
            "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (21.3)\n",
            "Collecting sphinxcontrib-qthelp\n",
            "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 10.5 MB/s \n",
            "\u001b[?25hCollecting sphinxcontrib-htmlhelp>=2.0.0\n",
            "  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 10.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel>=1.3->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.5.0->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from submodlib->-r distil/requirements/requirements.txt (line 11)) (0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r distil/requirements/requirements.txt (line 12)) (2.8.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r distil/requirements/requirements.txt (line 13)) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->Sphinx>=2.1->sphinxcontrib-bibtex>=2.3.0->-r distil/requirements/requirements.txt (line 1)) (3.0.6)\n",
            "Building wheels for collected packages: submodlib\n",
            "  Building wheel for submodlib (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for submodlib: filename=submodlib-1.1.5-cp37-cp37m-linux_x86_64.whl size=512655 sha256=2036c67a7f889b52807ece90ab036cd8b46c4470426c4468fa584739be47640e\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/3c/03/48ce7dd03798c0564b61e61020e733443aed88e115518442b4\n",
            "Successfully built submodlib\n",
            "Installing collected packages: latexcodec, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, scikit-learn, pybtex, Sphinx, pybtex-docutils, submodlib, sphinxcontrib-bibtex, multipledispatch\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "  Attempting uninstall: Sphinx\n",
            "    Found existing installation: Sphinx 1.8.6\n",
            "    Uninstalling Sphinx-1.8.6:\n",
            "      Successfully uninstalled Sphinx-1.8.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed Sphinx-4.3.1 latexcodec-2.0.1 multipledispatch-0.6.0 pybtex-0.24.0 pybtex-docutils-1.0.1 scikit-learn-0.23.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-bibtex-2.4.1 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 submodlib-1.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sphinxcontrib"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4oIq8U7f6Dw"
      },
      "source": [
        "## Preparing a Redundant CIFAR-10\n",
        "\n",
        "The CIFAR10 dataset contains 60,000 32x32 color images in 10 different classes.The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. The training set contains 50,000 images, and the test set contains 10,000 images. Here, we do a simple setup of the CIFAR10 dataset that we will use in this example. More importantly, we define a split on CIFAR10's training set into an initial labeled seed set and an unlabeled set. We also impose an artificial source of redundancy to simulate a redundancy setting."
      ],
      "id": "v4oIq8U7f6Dw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaaAWcsKf6Uv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "32b2a50fc68443ccb3b68df30e759c16",
            "5765a9a83d6b4080b2efe023690cda58",
            "d9faa4ce87b5492caaf2f7bd0266c6bc",
            "6db98e4e34df42608c906d36dba5dd0d",
            "8e0e3722ddb346858e2eaf6e48e111d5",
            "7f5065cbec814f3a8f64f00a539ddfd0",
            "08edb2d80422484c8958441915a601fe",
            "96f83d3a10874abd9b68f28ca4c48bc5",
            "9b7dd6f68bfa4b109a04e193fb2215d6",
            "bb9f212b37784774995dbfbac41b0dba",
            "2afbd98351bd49659a65b2a8d64c41b4"
          ]
        },
        "outputId": "0bdcabe1-d9d4-461e-b263-af6058fdf813"
      },
      "source": [
        "# Define the name of the dataset and the path that PyTorch should use when downloading the data\n",
        "data_set_name = 'CIFAR10'\n",
        "download_path = '.'\n",
        "\n",
        "# Define the number of classes in CIFAR10\n",
        "nclasses = 10\n",
        "\n",
        "# Define transforms on the dataset splits of CIFAR10. Here, we use random crops and horizontal flips for training augmentations.\n",
        "# Both the train and test sets are converted to PyTorch tensors and are normalized around the mean/std of CIFAR-10.\n",
        "cifar_training_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "# Get the dataset objects from PyTorch. Here, CIFAR10 is downloaded, and the transform is applied when points \n",
        "# are retrieved.\n",
        "cifar10_full_train = cifar.CIFAR10(download_path, train=True, download=True, transform=cifar_training_transform)\n",
        "cifar10_test = cifar.CIFAR10(download_path, train=False, download=True, transform=cifar_test_transform)\n",
        "\n",
        "# Get the dimension of the images. Here, we simply take the very first image of CIFAR10 \n",
        "# and query its dimension.\n",
        "dim = np.shape(cifar10_full_train[0][0])\n",
        "\n",
        "# We now define a train-unlabeled split for the sake of the experiment. Here, we randomly initialize a \n",
        "# seed set with 500 points.\n",
        "initial_seed_size = 100\n",
        "\n",
        "# We then create an unlabeled dataset with duplicated points. Here, we assume the setting where the initial \n",
        "# seed set was drawn from a redundant unlabeled set, so some of the points in the unlabeled dataset are those \n",
        "# already selected for the seed set. We also add some other examples that are redundant but were not selected \n",
        "# for the seed set. We duplicate everything 10 times, so there is quite a bit of redundancy.\n",
        "other_duplicated_examples_in_unlabeled_dataset = 1000\n",
        "duplication_factor = 10\n",
        "\n",
        "# Select indices for the train dataset\n",
        "index_bank = list(range(len(cifar10_full_train)))\n",
        "train_idx = list(np.random.choice(index_bank, size=initial_seed_size, replace=False))\n",
        "index_bank = list(set(index_bank) - set(train_idx))\n",
        "\n",
        "# Select indices for the unique and duplicated portions of the unlabeled dataset\n",
        "other_duplicated_unlabeled_idx = list(np.random.choice(index_bank, size=other_duplicated_examples_in_unlabeled_dataset, replace=False))\n",
        "\n",
        "# Create the unlabeled_idx by repeatedly adding the duplicated_unlabeled_idx\n",
        "unlabeled_idx = train_idx * duplication_factor + other_duplicated_unlabeled_idx * duplication_factor\n",
        "\n",
        "# Create the train and unlabeled subsets based on the index lists above. While the unlabeled set constructed here technically has labels, they \n",
        "# are only used when querying for labels. Hence, they only exist here for the sake of experimental design.\n",
        "cifar10_train = Subset(cifar10_full_train, train_idx)\n",
        "cifar10_unlabeled = Subset(cifar10_full_train, unlabeled_idx)"
      ],
      "id": "uaaAWcsKf6Uv",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32b2a50fc68443ccb3b68df30e759c16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./cifar-10-python.tar.gz to .\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjVjSm-slI7U"
      },
      "source": [
        "## Preparing the Model\n",
        "\n",
        "Here, we use DISTIL's provided implementation of the [ResNet-18](https://arxiv.org/abs/1512.03385) architecture. We also create a model directory to store trained models in this example."
      ],
      "id": "mjVjSm-slI7U"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjHfyJvslJID"
      },
      "source": [
        "net = ResNet18()"
      ],
      "id": "LjHfyJvslJID",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fsSP89pl7Rk"
      },
      "source": [
        "# Initial Round"
      ],
      "id": "2fsSP89pl7Rk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGAnKw8AoE_q"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now that we have prepared the training data and the model, we can begin training an initial model. We use DISTIL's provided [training loop](https://github.com/decile-team/distil/blob/main/distil/utils/train_helper.py) to do training."
      ],
      "id": "eGAnKw8AoE_q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6_PV51rl7f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f29a7b3-ffc4-45a7-c7d4-e9ec52d93aa4"
      },
      "source": [
        "# Define the training arguments to use.\n",
        "args = {'n_epoch':      300,    # Stop training after 300 epochs.\n",
        "        'lr':           0.01,   # Use a learning rate of 0.01\n",
        "        'batch_size':   20,     # Update the parameters using training batches of size 20\n",
        "        'max_accuracy': 0.99,   # Stop training once the training accuracy has exceeded 0.99\n",
        "        'optimizer':    'sgd',  # Use the stochastic gradient descent optimizer\n",
        "        'device':       \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use a GPU if one is available\n",
        "        }\n",
        "\n",
        "# Create the training loop using our training dataset, provided model, and training arguments.\n",
        "# Train an initial model.\n",
        "dt = data_train(cifar10_train, net, args)\n",
        "trained_model = dt.train()"
      ],
      "id": "f6_PV51rl7f0",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training..\n",
            "Epoch: 85 Training accuracy: 0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY13YA4GoIvY"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "How does our initial model do on the test set? Luckily, the training loop provided by DISTIL also provides a way to measure the accuracy of the model on a given dataset."
      ],
      "id": "yY13YA4GoIvY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGgL9oagoqKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc8907b-55da-4a29-e6d8-6e751ef3b2cc"
      },
      "source": [
        "# Get the full test accuracy\n",
        "full_test_accuracy = dt.get_acc_on_set(cifar10_test)\n",
        "print(F\"Full Test Accuracy: {full_test_accuracy}\")"
      ],
      "id": "uGgL9oagoqKT",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Test Accuracy: 0.2582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-47FnQRsfkM"
      },
      "source": [
        "As we can see, the test performance could use some improvement. Let's add some new training examples to our training dataset."
      ],
      "id": "p-47FnQRsfkM"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnDJvyMWtCCa"
      },
      "source": [
        "# Selecting Non-Redundant Examples"
      ],
      "id": "JnDJvyMWtCCa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lH1qtKB4tK2v"
      },
      "source": [
        "## Preparing a Conditioning (Private) Set\n",
        "\n",
        "In this example, we know that the unlabeled dataset can help in improving our model performance if we add the right examples to our training dataset; however, the unlabeled dataset is plagued with redundancy. Here, we use [SIMILAR](https://arxiv.org/abs/2107.00717) to select non-redundant examples from the unlabeled set. SIMILAR requires access to a conditioning (private) set of points that it uses to determine which unlabeled points are non-redundant. Hence, we must first prepare this conditioning set.\n",
        "\n",
        "Where do we get the conditioning set? Quite simply, we can use our current training dataset as the conditioning set! This will encourage the selection of non-redundant points."
      ],
      "id": "lH1qtKB4tK2v"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQupSdbrur3_"
      },
      "source": [
        "conditioning_set = cifar10_train"
      ],
      "id": "MQupSdbrur3_",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgvh20tYviAr"
      },
      "source": [
        "## Using DISTIL's SIMILAR Implementation: SCG\n",
        "\n",
        "Now that we have the query set, we are ready to use DISTIL's implementation of SIMILAR. In particular, we use the submodular conditional gain [strategy](https://github.com/decile-team/distil/blob/main/distil/active_learning_strategies/scg.py) that is detailed in [SIMILAR](https://arxiv.org/abs/2107.00717). This will allow us to select a set of points to label within a specified budget $k$.\n",
        "\n",
        "Specifically, the strategy attempts to maximize the [submodular conditional gain](https://arxiv.org/abs/2006.15412) between a subset $\\mathcal{A}$ of size no greater than $k$ of the unlabeled dataset $\\mathcal{U}$ and the conditioning set $\\mathcal{P}$:\n",
        "\n",
        "\\begin{align}\n",
        "\\text{argmax}_{\\mathcal{A} \\subseteq \\mathcal{U}, |\\mathcal{A}|\\leq k} H_F(\\mathcal{A} | \\mathcal{P})\n",
        "\\end{align}\n",
        "\n",
        "where $F$ is a submodular set function."
      ],
      "id": "cgvh20tYviAr"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPKl0J3HyZZO"
      },
      "source": [
        "# Define arguments for SMI\n",
        "selection_strategy_args = {'device':        args['device'],       # Use the device used in training\n",
        "                           'batch_size':    args['batch_size'],   # Use the batch size used in training\n",
        "                           'scg_function':  'flcg',           # Use a log determinant function, which captures diversity information\n",
        "                           'metric':        'cosine',             # Use cosine similarity when determining the likeness of two data points\n",
        "                           'optimizer':     'LazyGreedy'          # When doing submodular maximization, use the lazy greedy optimizer\n",
        "                          }\n",
        "\n",
        "# Create the SMI selection strategy. Note: We remove the labels from the unlabeled portion of CIFAR-10 that we created earlier.\n",
        "# In a practical application, one would not have these labels a priori.\n",
        "selection_strategy = SCG(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), conditioning_set, trained_model, nclasses, selection_strategy_args)\n",
        "\n",
        "# Disable the augmentations used in the training dataset. Since all augmentations come from the cifar10_full_train object, we set its transform to the \n",
        "# transform used by the test set.\n",
        "cifar10_full_train.transform = cifar_test_transform\n",
        "\n",
        "# Do the selection, which will return the indices of the selected points with respect to the unlabeled dataset.\n",
        "budget = 1000\n",
        "selected_idx = selection_strategy.select(budget)\n",
        "\n",
        "# Re-enable augmentations\n",
        "cifar10_full_train.transform = cifar_training_transform"
      ],
      "id": "HPKl0J3HyZZO",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoQAIxhA1dLE"
      },
      "source": [
        "## Labeling the Points\n",
        "\n",
        "Now that we know which points should be labeled, we can present them to human labelers for annotation. Here, we can do so automatically since we already know their labels for the sake of the example."
      ],
      "id": "zoQAIxhA1dLE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAlQ5Fr51yS8"
      },
      "source": [
        "# Form a labeled subset of the unlabeled dataset. Again, we already have the labels, \n",
        "# so we simply take a subset. Note, however, that the selection was done without the \n",
        "# use of the labels and that we would normally not have these labels. Hence, the \n",
        "# following statement would usually require human effort to complete.\n",
        "scg_human_labeled_dataset = Subset(cifar10_unlabeled, selected_idx)"
      ],
      "id": "HAlQ5Fr51yS8",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zom1rZrJ2Wi_"
      },
      "source": [
        "## Characterizing the Selection\n",
        "\n",
        "Now that we have selected and labeled these new points, we can add them to the training dataset and retrain our model. Before that, how many points did we select that were not repeats?"
      ],
      "id": "Zom1rZrJ2Wi_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN22GOF-2W1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4ac612f-046b-4751-8933-3e2fef851901"
      },
      "source": [
        "# selected_idx contains indices of the unlabeled dataset. For measurement purposes, we want to get the full dataset indices \n",
        "# that correspond to the indices of the unlabeled dataset.\n",
        "full_indices = list(np.array(unlabeled_idx)[selected_idx])\n",
        "\n",
        "# Get the unique points in the list that also are not in train_idx\n",
        "unique_points = len(set(full_indices) - set(train_idx))\n",
        "print(\"Unique Points Selected:\", unique_points)\n",
        "print(\"Unique Fraction:\", unique_points / budget)"
      ],
      "id": "GN22GOF-2W1M",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Points Selected: 998\n",
            "Unique Fraction: 0.998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyZx_yx07sEw"
      },
      "source": [
        "We were able to get a good number of unique points. For comparison sake, how many unique points would we get using some of DISTIL's other strategies?"
      ],
      "id": "xyZx_yx07sEw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvDNvY3D9hSn"
      },
      "source": [
        "**BADGE**"
      ],
      "id": "FvDNvY3D9hSn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN_iPljF9e_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abdc71d-40a0-473a-bae9-cbbd3be2398a"
      },
      "source": [
        "# Repeat the previous steps\n",
        "selection_strategy = BADGE(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), trained_model, nclasses, selection_strategy_args)\n",
        "\n",
        "cifar10_full_train.transform = cifar_test_transform\n",
        "budget = 1000\n",
        "selected_idx = selection_strategy.select(budget)\n",
        "cifar10_full_train.transform = cifar_training_transform\n",
        "\n",
        "badge_human_labeled_dataset = Subset(cifar10_unlabeled, selected_idx)\n",
        "\n",
        "full_indices = list(np.array(unlabeled_idx)[selected_idx])\n",
        "\n",
        "# Get the unique points in the list that also are not in train_idx\n",
        "unique_points = len(set(full_indices) - set(train_idx))\n",
        "print(\"Unique Points Selected:\", unique_points)\n",
        "print(\"Unique Fraction:\", unique_points / budget)"
      ],
      "id": "HN_iPljF9e_L",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Points Selected: 940\n",
            "Unique Fraction: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1PDZ2qk-hi7"
      },
      "source": [
        "**Random**"
      ],
      "id": "E1PDZ2qk-hi7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4DQ3_ld-j8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15d2baaa-c179-4a3b-cf23-a8b11cfd0511"
      },
      "source": [
        "# Repeat the previous steps\n",
        "selection_strategy = RandomSampling(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), trained_model, nclasses, selection_strategy_args)\n",
        "\n",
        "cifar10_full_train.transform = cifar_test_transform\n",
        "budget = 1000\n",
        "selected_idx = selection_strategy.select(budget)\n",
        "cifar10_full_train.transform = cifar_training_transform\n",
        "\n",
        "random_human_labeled_dataset = Subset(cifar10_unlabeled, selected_idx)\n",
        "\n",
        "full_indices = list(np.array(unlabeled_idx)[selected_idx])\n",
        "\n",
        "# Get the unique points in the list that also are not in train_idx\n",
        "unique_points = len(set(full_indices) - set(train_idx))\n",
        "print(\"Unique Points Selected:\", unique_points)\n",
        "print(\"Unique Fraction:\", unique_points / budget)"
      ],
      "id": "o4DQ3_ld-j8U",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Points Selected: 600\n",
            "Unique Fraction: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK5KB_UH-zR2"
      },
      "source": [
        "**Entropy**"
      ],
      "id": "wK5KB_UH-zR2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncA_-JMA-zoA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d03fe69-852f-4e99-8e08-d74ff3f2ca7c"
      },
      "source": [
        "# Repeat the previous steps\n",
        "selection_strategy = EntropySampling(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), trained_model, nclasses, selection_strategy_args)\n",
        "\n",
        "cifar10_full_train.transform = cifar_test_transform\n",
        "budget = 1000\n",
        "selected_idx = selection_strategy.select(budget)\n",
        "cifar10_full_train.transform = cifar_training_transform\n",
        "\n",
        "entropy_human_labeled_dataset = Subset(cifar10_unlabeled, selected_idx)\n",
        "\n",
        "full_indices = list(np.array(unlabeled_idx)[selected_idx])\n",
        "\n",
        "# Get the unique points in the list that also are not in train_idx\n",
        "unique_points = len(set(full_indices) - set(train_idx))\n",
        "print(\"Unique Points Selected:\", unique_points)\n",
        "print(\"Unique Fraction:\", unique_points / budget)"
      ],
      "id": "ncA_-JMA-zoA",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Points Selected: 99\n",
            "Unique Fraction: 0.099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvMg93--Byvl"
      },
      "source": [
        "Hence, we can see that SCG does better at selecting non-redundant examples."
      ],
      "id": "RvMg93--Byvl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjvhIDHsAkU1"
      },
      "source": [
        "# Improving Performance"
      ],
      "id": "vjvhIDHsAkU1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-HsA27AsUG"
      },
      "source": [
        "## Re-Training\n",
        "\n",
        "Let us re-train our model using the newly selected points."
      ],
      "id": "eF-HsA27AsUG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kkbzgxVAkl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e881d74a-59a2-4ca4-e76c-ee15a3cf14e7"
      },
      "source": [
        "# Create a new training dataset by concatenating what we have with the newly labeled points\n",
        "new_training_dataset = ConcatDataset([cifar10_train, scg_human_labeled_dataset])\n",
        "new_dt = data_train(new_training_dataset, copy.deepcopy(net), args)\n",
        "new_trained_model = new_dt.train()"
      ],
      "id": "1kkbzgxVAkl7",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training..\n",
            "Epoch: 115 Training accuracy: 0.995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PtUxT2pBLx_"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Now, let us see the accuracy improvement."
      ],
      "id": "6PtUxT2pBLx_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xj8wmgPBP9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e710f3b0-cd0a-439b-ac6b-6c2bc1bcd5e0"
      },
      "source": [
        "# Get the full test accuracy\n",
        "full_test_accuracy_before = dt.get_acc_on_set(cifar10_test)\n",
        "full_test_accuracy_after = new_dt.get_acc_on_set(cifar10_test)\n",
        "\n",
        "print(F\"Full Test Accuracy Improvement: {full_test_accuracy_before} to {full_test_accuracy_after}\")"
      ],
      "id": "2Xj8wmgPBP9U",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Test Accuracy Improvement: 0.2582 to 0.557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK89JhX6Fq3s"
      },
      "source": [
        "## Comparison\n",
        "\n",
        "What would the accuracy improvement look like if we had used the other methods?"
      ],
      "id": "BK89JhX6Fq3s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxklRX4uF1UO"
      },
      "source": [
        "**BADGE**"
      ],
      "id": "jxklRX4uF1UO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0_1FoFYF8pU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77cb8586-c37c-4880-e4b3-a4f6993d5a8f"
      },
      "source": [
        "# Repeat the process\n",
        "new_training_dataset = ConcatDataset([cifar10_train, badge_human_labeled_dataset])\n",
        "new_dt = data_train(new_training_dataset, copy.deepcopy(net), args)\n",
        "new_trained_model = new_dt.train()\n",
        "\n",
        "full_test_accuracy_before = dt.get_acc_on_set(cifar10_test)\n",
        "full_test_accuracy_after = new_dt.get_acc_on_set(cifar10_test)\n",
        "\n",
        "print(F\"Full Test Accuracy Improvement: {full_test_accuracy_before} to {full_test_accuracy_after}\")"
      ],
      "id": "X0_1FoFYF8pU",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training..\n",
            "Epoch: 109 Training accuracy: 0.992\n",
            "Full Test Accuracy Improvement: 0.2582 to 0.5466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNArqgcGF41l"
      },
      "source": [
        "**Random**"
      ],
      "id": "gNArqgcGF41l"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUgD2s2lF86O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf66381-5138-438c-ad87-4700d0688de8"
      },
      "source": [
        "# Repeat the process\n",
        "new_training_dataset = ConcatDataset([cifar10_train, random_human_labeled_dataset])\n",
        "new_dt = data_train(new_training_dataset, copy.deepcopy(net), args)\n",
        "new_trained_model = new_dt.train()\n",
        "\n",
        "full_test_accuracy_before = dt.get_acc_on_set(cifar10_test)\n",
        "full_test_accuracy_after = new_dt.get_acc_on_set(cifar10_test)\n",
        "\n",
        "print(F\"Full Test Accuracy Improvement: {full_test_accuracy_before} to {full_test_accuracy_after}\")"
      ],
      "id": "xUgD2s2lF86O",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training..\n",
            "Epoch: 92 Training accuracy: 0.991\n",
            "Full Test Accuracy Improvement: 0.2582 to 0.4695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkcyKstqF5Ml"
      },
      "source": [
        "**Entropy**"
      ],
      "id": "SkcyKstqF5Ml"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlB34d8fF9bP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9dec030-0907-4c92-ec93-c2c9dde356df"
      },
      "source": [
        "# Repeat the process\n",
        "new_training_dataset = ConcatDataset([cifar10_train, entropy_human_labeled_dataset])\n",
        "new_dt = data_train(new_training_dataset, copy.deepcopy(net), args)\n",
        "new_trained_model = new_dt.train()\n",
        "\n",
        "full_test_accuracy_before = dt.get_acc_on_set(cifar10_test)\n",
        "full_test_accuracy_after = new_dt.get_acc_on_set(cifar10_test)\n",
        "\n",
        "print(F\"Full Test Accuracy Improvement: {full_test_accuracy_before} to {full_test_accuracy_after}\")"
      ],
      "id": "nlB34d8fF9bP",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training..\n",
            "Epoch: 37 Training accuracy: 0.995\n",
            "Full Test Accuracy Improvement: 0.2582 to 0.2928\n"
          ]
        }
      ]
    }
  ]
}
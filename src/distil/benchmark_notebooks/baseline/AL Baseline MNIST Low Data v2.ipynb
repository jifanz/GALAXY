{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AL Baseline MNIST Low Data v2.ipynb","provenance":[{"file_id":"1mHB_gVjuOeJuT64usKivLjdKemf_10hx","timestamp":1630036883710},{"file_id":"1KTT3jTYNz4LLeucQ3GgvhzE-uQI2ahmx","timestamp":1630036412443},{"file_id":"1UCgt2EAAB8Hp66DlExwXSvt2tKulkiYL","timestamp":1629519496115},{"file_id":"1FWNnsx_PYfYBwJNhTgWo0sbhYL5pp0C5","timestamp":1629511363630},{"file_id":"1PNLcgRsEXkhO1OzohwgWbBpfRJ0OmN3_","timestamp":1618358247032},{"file_id":"1xdptoAFvVFjNGE2DjUD0VmPIW1x5P_zH","timestamp":1617574256755}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vYmMCnfg1PN8"},"source":["# Preface\n","\n","The locations requiring configuration for your experiment are commented in capital text."]},{"cell_type":"markdown","metadata":{"id":"kgYWNPhf801A"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"-7DmzUo2vZZ_"},"source":["## Installations"]},{"cell_type":"code","metadata":{"id":"wKMPt_L5bNeu"},"source":["!pip install sphinxcontrib-napoleon\n","!pip install sphinxcontrib-bibtex\n","!pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ submodlib\n","\n","!git clone https://github.com/decile-team/distil.git\n","!git clone https://github.com/circulosmeos/gdown.pl.git\n","\n","import sys\n","sys.path.append(\"/content/distil/\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYsutkIJrGvK"},"source":["**Experiment-Specific Imports**"]},{"cell_type":"code","metadata":{"id":"lfQKdd0DrKsa"},"source":["from distil.utils.models.mnist_net import MnistNet                                # IMPORT YOUR MODEL HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Maz6VJxS787x"},"source":["## Main Imports"]},{"cell_type":"code","metadata":{"id":"V9-8qRo8KD3a"},"source":["import pandas as pd \n","import numpy as np\n","import copy\n","from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n","import torch.nn.functional as F\n","from torch import nn\n","from torchvision import transforms\n","from torchvision import datasets\n","from PIL import Image\n","import torch\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import sys\n","sys.path.append('../')\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","import random\n","import os\n","import pickle\n","\n","from numpy.linalg import cond\n","from numpy.linalg import inv\n","from numpy.linalg import norm\n","from scipy import sparse as sp\n","from scipy.linalg import lstsq\n","from scipy.linalg import solve\n","from scipy.optimize import nnls\n","\n","from distil.active_learning_strategies.badge import BADGE\n","from distil.active_learning_strategies.glister import GLISTER\n","from distil.active_learning_strategies.margin_sampling import MarginSampling\n","from distil.active_learning_strategies.entropy_sampling import EntropySampling\n","from distil.active_learning_strategies.random_sampling import RandomSampling\n","from distil.active_learning_strategies.gradmatch_active import GradMatchActive\n","from distil.active_learning_strategies.fass import FASS\n","from distil.active_learning_strategies.adversarial_bim import AdversarialBIM\n","from distil.active_learning_strategies.adversarial_deepfool import AdversarialDeepFool\n","from distil.active_learning_strategies.core_set import CoreSet\n","from distil.active_learning_strategies.least_confidence_sampling import LeastConfidenceSampling\n","from distil.active_learning_strategies.margin_sampling import MarginSampling\n","from distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout import BALDDropout\n","from distil.active_learning_strategies.batch_bald import BatchBALDDropout\n","from distil.utils.train_helper import data_train\n","from distil.utils.utils import LabeledToUnlabeledDataset\n","\n","from google.colab import drive\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecvumggc6UhF"},"source":["## Checkpointing and Logs"]},{"cell_type":"code","metadata":{"id":"ohuLHm5E58bj"},"source":["class Checkpoint:\n","\n","    def __init__(self, acc_list=None, indices=None, state_dict=None, experiment_name=None, path=None):\n","\n","        # If a path is supplied, load a checkpoint from there.\n","        if path is not None:\n","\n","            if experiment_name is not None:\n","                self.load_checkpoint(path, experiment_name)\n","            else:\n","                raise ValueError(\"Checkpoint contains None value for experiment_name\")\n","\n","            return\n","\n","        if acc_list is None:\n","            raise ValueError(\"Checkpoint contains None value for acc_list\")\n","\n","        if indices is None:\n","            raise ValueError(\"Checkpoint contains None value for indices\")\n","\n","        if state_dict is None:\n","            raise ValueError(\"Checkpoint contains None value for state_dict\")\n","\n","        if experiment_name is None:\n","            raise ValueError(\"Checkpoint contains None value for experiment_name\")\n","\n","        self.acc_list = acc_list\n","        self.indices = indices\n","        self.state_dict = state_dict\n","        self.experiment_name = experiment_name\n","\n","    def __eq__(self, other):\n","\n","        # Check if the accuracy lists are equal\n","        acc_lists_equal = self.acc_list == other.acc_list\n","\n","        # Check if the indices are equal\n","        indices_equal = self.indices == other.indices\n","\n","        # Check if the experiment names are equal\n","        experiment_names_equal = self.experiment_name == other.experiment_name\n","\n","        return acc_lists_equal and indices_equal and experiment_names_equal\n","\n","    def save_checkpoint(self, path):\n","\n","        # Get current time to use in file timestamp\n","        timestamp = time.time_ns()\n","\n","        # Create the path supplied\n","        os.makedirs(path, exist_ok=True)\n","\n","        # Name saved files using timestamp to add recency information\n","        save_path = os.path.join(path, F\"c{timestamp}1\")\n","        copy_save_path = os.path.join(path, F\"c{timestamp}2\")\n","\n","        # Write this checkpoint to the first save location\n","        with open(save_path, 'wb') as save_file:\n","            pickle.dump(self, save_file)\n","\n","        # Write this checkpoint to the second save location\n","        with open(copy_save_path, 'wb') as copy_save_file:\n","            pickle.dump(self, copy_save_file)\n","\n","    def load_checkpoint(self, path, experiment_name):\n","\n","        # Obtain a list of all files present at the path\n","        timestamp_save_no = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n","\n","        # If there are no such files, set values to None and return\n","        if len(timestamp_save_no) == 0:\n","            self.acc_list = None\n","            self.indices = None\n","            self.state_dict = None\n","            return\n","\n","        # Sort the list of strings to get the most recent\n","        timestamp_save_no.sort(reverse=True)\n","\n","        # Read in two files at a time, checking if they are equal to one another. \n","        # If they are equal, then it means that the save operation finished correctly.\n","        # If they are not, then it means that the save operation failed (could not be \n","        # done atomically). Repeat this action until no possible pair can exist.\n","        while len(timestamp_save_no) > 1:\n","\n","            # Pop a most recent checkpoint copy\n","            first_file = timestamp_save_no.pop(0)\n","\n","            # Keep popping until two copies with equal timestamps are present\n","            while True:\n","                \n","                second_file = timestamp_save_no.pop(0)\n","                \n","                # Timestamps match if the removal of the \"1\" or \"2\" results in equal numbers\n","                if (second_file[:-1]) == (first_file[:-1]):\n","                    break\n","                else:\n","                    first_file = second_file\n","\n","                    # If there are no more checkpoints to examine, set to None and return\n","                    if len(timestamp_save_no) == 0:\n","                        self.acc_list = None\n","                        self.indices = None\n","                        self.state_dict = None\n","                        return\n","\n","            # Form the paths to the files\n","            load_path = os.path.join(path, first_file)\n","            copy_load_path = os.path.join(path, second_file)\n","\n","            # Load the two checkpoints\n","            with open(load_path, 'rb') as load_file:\n","                checkpoint = pickle.load(load_file)\n","\n","            with open(copy_load_path, 'rb') as copy_load_file:\n","                checkpoint_copy = pickle.load(copy_load_file)\n","\n","            # Do not check this experiment if it is not the one we need to restore\n","            if checkpoint.experiment_name != experiment_name:\n","                continue\n","\n","            # Check if they are equal\n","            if checkpoint == checkpoint_copy:\n","\n","                # This checkpoint will suffice. Populate this checkpoint's fields \n","                # with the selected checkpoint's fields.\n","                self.acc_list = checkpoint.acc_list\n","                self.indices = checkpoint.indices\n","                self.state_dict = checkpoint.state_dict\n","                return\n","\n","        # Instantiate None values in acc_list, indices, and model\n","        self.acc_list = None\n","        self.indices = None\n","        self.state_dict = None\n","\n","    def get_saved_values(self):\n","\n","        return (self.acc_list, self.indices, self.state_dict)\n","\n","def delete_checkpoints(checkpoint_directory, experiment_name):\n","\n","    # Iteratively go through each checkpoint, deleting those whose experiment name matches.\n","    timestamp_save_no = [f for f in os.listdir(checkpoint_directory) if os.path.isfile(os.path.join(checkpoint_directory, f))]\n","\n","    for file in timestamp_save_no:\n","\n","        delete_file = False\n","\n","        # Get file location\n","        file_path = os.path.join(checkpoint_directory, file)\n","\n","        if not os.path.exists(file_path):\n","            continue\n","\n","        # Unpickle the checkpoint and see if its experiment name matches\n","        with open(file_path, \"rb\") as load_file:\n","\n","            checkpoint_copy = pickle.load(load_file)\n","            if checkpoint_copy.experiment_name == experiment_name:\n","                delete_file = True\n","\n","        # Delete this file only if the experiment name matched\n","        if delete_file:\n","            os.remove(file_path)\n","\n","#Logs\n","def write_logs(logs, save_directory, rd):\n","  file_path = save_directory + 'run_'+'.txt'\n","  with open(file_path, 'a') as f:\n","    f.write('---------------------\\n')\n","    f.write('Round '+str(rd)+'\\n')\n","    f.write('---------------------\\n')\n","    for key, val in logs.items():\n","      if key == 'Training':\n","        f.write(str(key)+ '\\n')\n","        for epoch in val:\n","          f.write(str(epoch)+'\\n')       \n","      else:\n","        f.write(str(key) + ' - '+ str(val) +'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bd-8vetN6cP-"},"source":["## AL Loop"]},{"cell_type":"code","metadata":{"id":"jWdgKV2M6PFu"},"source":["def train_one(full_train_dataset, initial_train_indices, test_dataset, net, n_rounds, budget, args, nclasses, strategy, save_directory, checkpoint_directory, experiment_name):\n","\n","    # Split the full training dataset into an initial training dataset and an unlabeled dataset\n","    train_dataset = Subset(full_train_dataset, initial_train_indices)\n","    initial_unlabeled_indices = list(set(range(len(full_train_dataset))) - set(initial_train_indices))\n","    unlabeled_dataset = Subset(full_train_dataset, initial_unlabeled_indices)\n","\n","    # Set up the AL strategy\n","    if strategy == \"random\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = RandomSampling(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"entropy\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = EntropySampling(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"margin\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = MarginSampling(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"least_confidence\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = LeastConfidenceSampling(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"badge\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = BADGE(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"coreset\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = CoreSet(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"fass\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = FASS(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"glister\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'lr': args['lr'], 'device':args['device']}\n","        strategy = GLISTER(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args, typeOf='rand', lam=0.1)\n","    elif strategy == \"adversarial_bim\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = AdversarialBIM(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"adversarial_deepfool\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = AdversarialDeepFool(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"bald\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = BALDDropout(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"batch_bald\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device'], 'mod_inject':args['mod_inject']}\n","        strategy = BatchBALDDropout(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","\n","    # Define acc initially\n","    acc = np.zeros(n_rounds+1)\n","\n","    initial_unlabeled_size = len(unlabeled_dataset)\n","\n","    initial_round = 1\n","\n","    # Define an index map\n","    index_map = np.array([x for x in range(initial_unlabeled_size)])\n","\n","    # Attempt to load a checkpoint. If one exists, then the experiment crashed.\n","    training_checkpoint = Checkpoint(experiment_name=experiment_name, path=checkpoint_directory)\n","    rec_acc, rec_indices, rec_state_dict = training_checkpoint.get_saved_values()\n","\n","    # Check if there are values to recover\n","    if rec_acc is not None:\n","\n","        # Restore the accuracy list\n","        for i in range(len(rec_acc)):\n","            acc[i] = rec_acc[i]\n","\n","        # Restore the indices list and shift those unlabeled points to the labeled set.\n","        index_map = np.delete(index_map, rec_indices)\n","\n","        # Record initial size of the training dataset\n","        intial_seed_size = len(train_dataset)\n","\n","        restored_unlabeled_points = Subset(unlabeled_dataset, rec_indices)\n","        train_dataset = ConcatDataset([train_dataset, restored_unlabeled_points])\n","\n","        remaining_unlabeled_indices = list(set(range(len(unlabeled_dataset))) - set(rec_indices))\n","        unlabeled_dataset = Subset(unlabeled_dataset, remaining_unlabeled_indices)\n","\n","        # Restore the model\n","        net.load_state_dict(rec_state_dict) \n","\n","        # Fix the initial round\n","        initial_round = (len(train_dataset) - initial_seed_size) // budget + 1\n","\n","        # Ensure loaded model is moved to GPU\n","        if torch.cuda.is_available():\n","            net = net.cuda()     \n","\n","        strategy.update_model(net)\n","        strategy.update_data(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset)) \n","\n","        dt = data_train(train_dataset, net, args)\n","\n","    else:\n","\n","        if torch.cuda.is_available():\n","            net = net.cuda()\n","\n","        dt = data_train(train_dataset, net, args)\n","\n","        acc[0] = dt.get_acc_on_set(test_dataset)\n","        print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n","\n","        logs = {}\n","        logs['Training Points'] = len(train_dataset)\n","        logs['Test Accuracy'] =  str(round(acc[0]*100, 2))\n","        write_logs(logs, save_directory, 0)\n","          \n","        #Updating the trained model in strategy class\n","        strategy.update_model(net)\n","\n","    # Record the training transform and test transform for disabling purposes\n","    train_transform = full_train_dataset.transform\n","    test_transform = test_dataset.transform\n","\n","    ##User Controlled Loop\n","    for rd in range(initial_round, n_rounds+1):\n","        print('-------------------------------------------------')\n","        print('Round', rd) \n","        print('-------------------------------------------------')\n","\n","        sel_time = time.time()\n","        full_train_dataset.transform = test_transform # Disable any augmentation while selecting points\n","        idx = strategy.select(budget)            \n","        full_train_dataset.transform = train_transform # Re-enable any augmentation done during training\n","        sel_time = time.time() - sel_time\n","        print(\"Selection Time:\", sel_time)\n","\n","        selected_unlabeled_points = Subset(unlabeled_dataset, idx)\n","        train_dataset = ConcatDataset([train_dataset, selected_unlabeled_points])\n","\n","        remaining_unlabeled_indices = list(set(range(len(unlabeled_dataset))) - set(idx))\n","        unlabeled_dataset = Subset(unlabeled_dataset, remaining_unlabeled_indices)\n","\n","        # Update the index map\n","        index_map = np.delete(index_map, idx, axis = 0)\n","\n","        print('Number of training points -', len(train_dataset))\n","\n","        # Start training\n","        strategy.update_data(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset))\n","        dt.update_data(train_dataset)\n","        t1 = time.time()\n","        clf, train_logs = dt.train(None)\n","        t2 = time.time()\n","        acc[rd] = dt.get_acc_on_set(test_dataset)\n","        logs = {}\n","        logs['Training Points'] = len(train_dataset)\n","        logs['Test Accuracy'] =  str(round(acc[rd]*100, 2))\n","        logs['Selection Time'] = str(sel_time)\n","        logs['Trainining Time'] = str(t2 - t1) \n","        logs['Training'] = train_logs\n","        write_logs(logs, save_directory, rd)\n","        strategy.update_model(clf)\n","        print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n","\n","        # Create a checkpoint\n","        used_indices = np.array([x for x in range(initial_unlabeled_size)])\n","        used_indices = np.delete(used_indices, index_map).tolist()\n","\n","        round_checkpoint = Checkpoint(acc.tolist(), used_indices, clf.state_dict(), experiment_name=experiment_name)\n","        round_checkpoint.save_checkpoint(checkpoint_directory)\n","\n","    print('Training Completed')\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-rFh9y0M3ZVH"},"source":["# MNIST"]},{"cell_type":"markdown","metadata":{"id":"E-e_sDnGsC_N"},"source":["## Parameter Definitions\n","\n","Parameters related to the specific experiment are placed here. You should examine each and modify them as needed."]},{"cell_type":"code","metadata":{"id":"0cHXLa_YsIQG"},"source":["data_set_name = \"MNIST\" # DSET NAME HERE\n","dataset_root_path = '../downloaded_data/'\n","net = MnistNet() # MODEL HERE\n","\n","# MODIFY AS NECESSARY\n","logs_directory = '/content/gdrive/MyDrive/colab_storage/logs/'\n","checkpoint_directory = '/content/gdrive/MyDrive/colab_storage/check/'\n","model_directory = \"/content/gdrive/MyDrive/colab_storage/model/\"\n","\n","experiment_name = \"MNIST LOW DATA BASELINE\"\n","\n","initial_seed_size = 50 # INIT SEED SIZE HERE\n","training_size_cap = 150 # TRAIN SIZE CAP HERE\n","\n","budget = 10 # BUDGET HERE\n","\n","# CHANGE ARGS AS NECESSARY\n","args = {'n_epoch':3000, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':float(0.99), 'islogs':True, 'isreset':True, 'isverbose':True, 'device':'cuda', 'mod_inject':'fc2', 'n_samples': 1000, 'n_drop': 50} \n","\n","# Train on approximately the full dataset given the budget contraints\n","n_rounds = (training_size_cap - initial_seed_size) // budget"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O0WfH3eq3nv_"},"source":["## Initial Loading and Training\n","\n","You may choose to train a new initial model or to continue to load a specific model. If this notebook is being executed in Colab, you should consider whether or not you need the gdown line."]},{"cell_type":"code","metadata":{"id":"K1522SUk3nwF"},"source":["# Mount drive containing possible saved model and define file path.\n","colab_model_storage_mount = \"/content/gdrive\"\n","drive.mount(colab_model_storage_mount)\n","\n","# Retrieve the model from a download link and save it to the drive\n","os.makedirs(logs_directory, exist_ok = True)\n","os.makedirs(checkpoint_directory, exist_ok = True)\n","os.makedirs(model_directory, exist_ok = True)\n","model_directory = F\"{model_directory}/{data_set_name}\"\n","#!/content/gdown.pl/gdown.pl \"INSERT SHARABLE LINK HERE\" \"INSERT DOWNLOAD LOCATION HERE (ideally, same as model_directory)\" # MAY NOT NEED THIS LINE IF NOT CLONING MODEL FROM COLAB\n","\n","# Load the dataset\n","if data_set_name == \"CIFAR10\":\n","\n","    train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n","    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n","\n","    full_train_dataset = datasets.CIFAR10(dataset_root_path, download=True, train=True, transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.CIFAR10(dataset_root_path, download=True, train=False, transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 10 # NUM CLASSES HERE\n","\n","elif data_set_name == \"CIFAR100\":\n","\n","    train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n","    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n","\n","    full_train_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=True, transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=False, transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 100 # NUM CLASSES HERE\n","\n","elif data_set_name == \"MNIST\":\n","\n","    image_dim=28\n","    train_transform = transforms.Compose([transforms.RandomCrop(image_dim, padding=4), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","    test_transform = transforms.Compose([transforms.Resize((image_dim, image_dim)), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","\n","    full_train_dataset = datasets.MNIST(dataset_root_path, download=True, train=True, transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.MNIST(dataset_root_path, download=True, train=False, transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 10 # NUM CLASSES HERE\n","\n","elif data_set_name == \"FashionMNIST\":\n","\n","    train_transform = transforms.Compose([transforms.RandomCrop(28, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # Use mean/std of MNIST\n","\n","    full_train_dataset = datasets.FashionMNIST(dataset_root_path, download=True, train=True, transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.FashionMNIST(dataset_root_path, download=True, train=False, transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 10 # NUM CLASSES HERE\n","\n","elif data_set_name == \"SVHN\":\n","\n","    train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n","    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) # ImageNet mean/std\n","\n","    full_train_dataset = datasets.SVHN(dataset_root_path, split='train', download=True, transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.SVHN(dataset_root_path, split='test', download=True, transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 10 # NUM CLASSES HERE\n","\n","elif data_set_name == \"ImageNet\":\n","\n","    train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n","    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) # ImageNet mean/std\n","\n","    # Note: Not automatically downloaded due to size restrictions. Notebook needs to be adapted to run on local device.\n","    full_train_dataset = datasets.ImageNet(dataset_root_path, download=False, split='train', transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.ImageNet(dataset_root_path, download=False, split='val', transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 1000 # NUM CLASSES HERE\n","\n","args['nclasses'] = nclasses\n","\n","dim = full_train_dataset[0][0].shape\n","\n","# Seed the random number generator for reproducibility and create the initial seed set\n","np.random.seed(42)\n","initial_train_indices = np.random.choice(len(full_train_dataset), replace=False, size=initial_seed_size)\n","\n","# COMMENT OUT ONE OR THE OTHER IF YOU WANT TO TRAIN A NEW INITIAL MODEL\n","load_model = False\n","#load_model = True\n","\n","# Only train a new model if one does not exist.\n","if load_model:\n","    net.load_state_dict(torch.load(model_directory))\n","    initial_model = net\n","else:\n","    dt = data_train(Subset(full_train_dataset, initial_train_indices), net, args)\n","    initial_model, _ = dt.train(None)\n","    torch.save(initial_model.state_dict(), model_directory)\n","\n","print(\"Training for\", n_rounds, \"rounds with budget\", budget, \"on unlabeled set size\", training_size_cap)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9N-4eTMPrZZ"},"source":["## Random Sampling"]},{"cell_type":"code","metadata":{"id":"i4eKSOaiPruO"},"source":["strategy = \"random\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bg1XH87hPsCe"},"source":["## Entropy"]},{"cell_type":"code","metadata":{"id":"mRAKMe2RPsTp"},"source":["strategy = \"entropy\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkMYHHwyP5Bd"},"source":["## GLISTER"]},{"cell_type":"code","metadata":{"id":"D0KstkZWP5fT"},"source":["strategy = \"glister\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AQmHTnnOP9KU"},"source":["## FASS"]},{"cell_type":"code","metadata":{"id":"tJSuDWowP9fD"},"source":["strategy = \"fass\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ZSiRahu3nwK"},"source":["## BADGE"]},{"cell_type":"code","metadata":{"id":"b5c8AckN3nwK"},"source":["strategy = \"badge\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RQeyff_gWp2E"},"source":["## CoreSet"]},{"cell_type":"code","metadata":{"id":"T4osHfoHWp2F"},"source":["strategy = \"coreset\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TwlszjoBWqMW"},"source":["## Least Confidence"]},{"cell_type":"code","metadata":{"id":"myi_zJ2UWqMW"},"source":["strategy = \"least_confidence\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rv-NjQBFWqWW"},"source":["## Margin"]},{"cell_type":"code","metadata":{"id":"IE0NVJW5WqWW"},"source":["strategy = \"margin\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ZIHlZAUxuga"},"source":["## Adversarial DeepFool"]},{"cell_type":"code","metadata":{"id":"skMF759xxuge"},"source":["strategy = \"adversarial_deepfool\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uESHCbJtxu1q"},"source":["## BALD"]},{"cell_type":"code","metadata":{"id":"ZSuL3xQCxu1q"},"source":["strategy = \"bald\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jpAaaz7Bxv35"},"source":["## BatchBALD"]},{"cell_type":"code","metadata":{"id":"qkTMho6exv35"},"source":["strategy = \"batch_bald\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AL SWA SS CIFAR10 v2.ipynb","provenance":[{"file_id":"1ynU4iEzsooDiAV548qxJWilq7OqFgiF6","timestamp":1630051587404},{"file_id":"1KTT3jTYNz4LLeucQ3GgvhzE-uQI2ahmx","timestamp":1630049089761},{"file_id":"1UCgt2EAAB8Hp66DlExwXSvt2tKulkiYL","timestamp":1629519496115},{"file_id":"1FWNnsx_PYfYBwJNhTgWo0sbhYL5pp0C5","timestamp":1629511363630},{"file_id":"1PNLcgRsEXkhO1OzohwgWbBpfRJ0OmN3_","timestamp":1618358247032},{"file_id":"1xdptoAFvVFjNGE2DjUD0VmPIW1x5P_zH","timestamp":1617574256755}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vYmMCnfg1PN8"},"source":["# Preface\n","\n","The locations requiring configuration for your experiment are commented in capital text."]},{"cell_type":"markdown","metadata":{"id":"kgYWNPhf801A"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"-7DmzUo2vZZ_"},"source":["## Installations"]},{"cell_type":"code","metadata":{"id":"wKMPt_L5bNeu"},"source":["!pip install sphinxcontrib-napoleon\n","!pip install sphinxcontrib-bibtex\n","!pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ submodlib\n","\n","!git clone https://github.com/decile-team/distil.git\n","!git clone https://github.com/circulosmeos/gdown.pl.git\n","!git clone https://github.com/owruby/shake-shake_pytorch.git\n","\n","!mv shake-shake_pytorch/models .\n","\n","import sys\n","sys.path.append(\"/content/distil/\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYsutkIJrGvK"},"source":["**Experiment-Specific Imports**"]},{"cell_type":"code","metadata":{"id":"lfQKdd0DrKsa"},"source":["from distil.utils.models.resnet import ResNet18                                 # IMPORT YOUR MODEL HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Maz6VJxS787x"},"source":["## Main Imports"]},{"cell_type":"code","metadata":{"id":"V9-8qRo8KD3a"},"source":["import pandas as pd \n","import numpy as np\n","import copy\n","from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n","import torch.nn.functional as F\n","from torch import nn\n","from torchvision import transforms\n","from torchvision import datasets\n","from PIL import Image\n","import torch\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import sys\n","sys.path.append('../')\n","import matplotlib.pyplot as plt\n","import time\n","import math\n","import random\n","import os\n","import pickle\n","\n","from numpy.linalg import cond\n","from numpy.linalg import inv\n","from numpy.linalg import norm\n","from scipy import sparse as sp\n","from scipy.linalg import lstsq\n","from scipy.linalg import solve\n","from scipy.optimize import nnls\n","\n","from distil.active_learning_strategies.badge import BADGE\n","from distil.active_learning_strategies.glister import GLISTER\n","from distil.active_learning_strategies.margin_sampling import MarginSampling\n","from distil.active_learning_strategies.entropy_sampling import EntropySampling\n","from distil.active_learning_strategies.random_sampling import RandomSampling\n","from distil.active_learning_strategies.gradmatch_active import GradMatchActive\n","from distil.active_learning_strategies.fass import FASS\n","from distil.active_learning_strategies.adversarial_bim import AdversarialBIM\n","from distil.active_learning_strategies.adversarial_deepfool import AdversarialDeepFool\n","from distil.active_learning_strategies.core_set import CoreSet\n","from distil.active_learning_strategies.least_confidence_sampling import LeastConfidenceSampling\n","from distil.active_learning_strategies.margin_sampling import MarginSampling\n","from distil.active_learning_strategies.bayesian_active_learning_disagreement_dropout import BALDDropout\n","from distil.utils.utils import LabeledToUnlabeledDataset\n","\n","from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n","\n","from models.shakeshake import ShakeShake\n","from models.shakeshake import Shortcut\n","\n","from google.colab import drive\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecvumggc6UhF"},"source":["## Checkpointing and Logs"]},{"cell_type":"code","metadata":{"id":"ohuLHm5E58bj"},"source":["class Checkpoint:\n","\n","    def __init__(self, acc_list=None, indices=None, state_dict=None, experiment_name=None, path=None):\n","\n","        # If a path is supplied, load a checkpoint from there.\n","        if path is not None:\n","\n","            if experiment_name is not None:\n","                self.load_checkpoint(path, experiment_name)\n","            else:\n","                raise ValueError(\"Checkpoint contains None value for experiment_name\")\n","\n","            return\n","\n","        if acc_list is None:\n","            raise ValueError(\"Checkpoint contains None value for acc_list\")\n","\n","        if indices is None:\n","            raise ValueError(\"Checkpoint contains None value for indices\")\n","\n","        if state_dict is None:\n","            raise ValueError(\"Checkpoint contains None value for state_dict\")\n","\n","        if experiment_name is None:\n","            raise ValueError(\"Checkpoint contains None value for experiment_name\")\n","\n","        self.acc_list = acc_list\n","        self.indices = indices\n","        self.state_dict = state_dict\n","        self.experiment_name = experiment_name\n","\n","    def __eq__(self, other):\n","\n","        # Check if the accuracy lists are equal\n","        acc_lists_equal = self.acc_list == other.acc_list\n","\n","        # Check if the indices are equal\n","        indices_equal = self.indices == other.indices\n","\n","        # Check if the experiment names are equal\n","        experiment_names_equal = self.experiment_name == other.experiment_name\n","\n","        return acc_lists_equal and indices_equal and experiment_names_equal\n","\n","    def save_checkpoint(self, path):\n","\n","        # Get current time to use in file timestamp\n","        timestamp = time.time_ns()\n","\n","        # Create the path supplied\n","        os.makedirs(path, exist_ok=True)\n","\n","        # Name saved files using timestamp to add recency information\n","        save_path = os.path.join(path, F\"c{timestamp}1\")\n","        copy_save_path = os.path.join(path, F\"c{timestamp}2\")\n","\n","        # Write this checkpoint to the first save location\n","        with open(save_path, 'wb') as save_file:\n","            pickle.dump(self, save_file)\n","\n","        # Write this checkpoint to the second save location\n","        with open(copy_save_path, 'wb') as copy_save_file:\n","            pickle.dump(self, copy_save_file)\n","\n","    def load_checkpoint(self, path, experiment_name):\n","\n","        # Obtain a list of all files present at the path\n","        timestamp_save_no = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n","\n","        # If there are no such files, set values to None and return\n","        if len(timestamp_save_no) == 0:\n","            self.acc_list = None\n","            self.indices = None\n","            self.state_dict = None\n","            return\n","\n","        # Sort the list of strings to get the most recent\n","        timestamp_save_no.sort(reverse=True)\n","\n","        # Read in two files at a time, checking if they are equal to one another. \n","        # If they are equal, then it means that the save operation finished correctly.\n","        # If they are not, then it means that the save operation failed (could not be \n","        # done atomically). Repeat this action until no possible pair can exist.\n","        while len(timestamp_save_no) > 1:\n","\n","            # Pop a most recent checkpoint copy\n","            first_file = timestamp_save_no.pop(0)\n","\n","            # Keep popping until two copies with equal timestamps are present\n","            while True:\n","                \n","                second_file = timestamp_save_no.pop(0)\n","                \n","                # Timestamps match if the removal of the \"1\" or \"2\" results in equal numbers\n","                if (second_file[:-1]) == (first_file[:-1]):\n","                    break\n","                else:\n","                    first_file = second_file\n","\n","                    # If there are no more checkpoints to examine, set to None and return\n","                    if len(timestamp_save_no) == 0:\n","                        self.acc_list = None\n","                        self.indices = None\n","                        self.state_dict = None\n","                        return\n","\n","            # Form the paths to the files\n","            load_path = os.path.join(path, first_file)\n","            copy_load_path = os.path.join(path, second_file)\n","\n","            # Load the two checkpoints\n","            with open(load_path, 'rb') as load_file:\n","                checkpoint = pickle.load(load_file)\n","\n","            with open(copy_load_path, 'rb') as copy_load_file:\n","                checkpoint_copy = pickle.load(copy_load_file)\n","\n","            # Do not check this experiment if it is not the one we need to restore\n","            if checkpoint.experiment_name != experiment_name:\n","                continue\n","\n","            # Check if they are equal\n","            if checkpoint == checkpoint_copy:\n","\n","                # This checkpoint will suffice. Populate this checkpoint's fields \n","                # with the selected checkpoint's fields.\n","                self.acc_list = checkpoint.acc_list\n","                self.indices = checkpoint.indices\n","                self.state_dict = checkpoint.state_dict\n","                return\n","\n","        # Instantiate None values in acc_list, indices, and model\n","        self.acc_list = None\n","        self.indices = None\n","        self.state_dict = None\n","\n","    def get_saved_values(self):\n","\n","        return (self.acc_list, self.indices, self.state_dict)\n","\n","def delete_checkpoints(checkpoint_directory, experiment_name):\n","\n","    # Iteratively go through each checkpoint, deleting those whose experiment name matches.\n","    timestamp_save_no = [f for f in os.listdir(checkpoint_directory) if os.path.isfile(os.path.join(checkpoint_directory, f))]\n","\n","    for file in timestamp_save_no:\n","\n","        delete_file = False\n","\n","        # Get file location\n","        file_path = os.path.join(checkpoint_directory, file)\n","\n","        if not os.path.exists(file_path):\n","            continue\n","\n","        # Unpickle the checkpoint and see if its experiment name matches\n","        with open(file_path, \"rb\") as load_file:\n","\n","            checkpoint_copy = pickle.load(load_file)\n","            if checkpoint_copy.experiment_name == experiment_name:\n","                delete_file = True\n","\n","        # Delete this file only if the experiment name matched\n","        if delete_file:\n","            os.remove(file_path)\n","\n","#Logs\n","def write_logs(logs, save_directory, rd):\n","  file_path = save_directory + 'run_'+'.txt'\n","  with open(file_path, 'a') as f:\n","    f.write('---------------------\\n')\n","    f.write('Round '+str(rd)+'\\n')\n","    f.write('---------------------\\n')\n","    for key, val in logs.items():\n","      if key == 'Training':\n","        f.write(str(key)+ '\\n')\n","        for epoch in val:\n","          f.write(str(epoch)+'\\n')       \n","      else:\n","        f.write(str(key) + ' - '+ str(val) +'\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0miCeJxzf5X8"},"source":["## Modified Training Class (SWA)"]},{"cell_type":"code","metadata":{"id":"In2w0jaif5n4"},"source":["def init_weights(m):\n","    if type(m) == nn.Linear:\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)\n","\n","class AddIndexDataset(Dataset):\n","    \n","    def __init__(self, wrapped_dataset):\n","        self.wrapped_dataset = wrapped_dataset\n","        \n","    def __getitem__(self, index):\n","        data, label = self.wrapped_dataset[index]\n","        return data, label, index\n","    \n","    def __len__(self):\n","        return len(self.wrapped_dataset)\n","\n","#custom training\n","class data_train:\n","\n","    \"\"\"\n","    Provides a configurable training loop for AL.\n","    \n","    Parameters\n","    ----------\n","    training_dataset: torch.utils.data.Dataset\n","        The training dataset to use\n","    net: torch.nn.Module\n","        The model to train\n","    args: dict\n","        Additional arguments to control the training loop\n","        \n","        `batch_size` - The size of each training batch (int, optional)\n","        `islogs`- Whether to return training metadata (bool, optional)\n","        `optimizer`- The choice of optimizer. Must be one of 'sgd' or 'adam' (string, optional)\n","        `isverbose`- Whether to print more messages about the training (bool, optional)\n","        `isreset`- Whether to reset the model before training (bool, optional)\n","        `max_accuracy`- The training accuracy cutoff by which to stop training (float, optional)\n","        `min_diff_acc`- The minimum difference in accuracy to measure in the window of monitored accuracies. If all differences are less than the minimum, stop training (float, optional)\n","        `window_size`- The size of the window for monitoring accuracies. If all differences are less than 'min_diff_acc', then stop training (int, optional)\n","        `criterion`- The criterion to use for training (typing.Callable[], optional)\n","        `device`- The device to use for training (string, optional)\n","    \"\"\"\n","    \n","    def __init__(self, training_dataset, net, args):\n","\n","        self.training_dataset = AddIndexDataset(training_dataset)\n","        self.net = net\n","        self.args = args\n","        \n","        self.n_pool = len(training_dataset)\n","        \n","        if 'islogs' not in args:\n","            self.args['islogs'] = False\n","\n","        if 'optimizer' not in args:\n","            self.args['optimizer'] = 'sgd'\n","        \n","        if 'isverbose' not in args:\n","            self.args['isverbose'] = False\n","        \n","        if 'isreset' not in args:\n","            self.args['isreset'] = True\n","\n","        if 'max_accuracy' not in args:\n","            self.args['max_accuracy'] = 0.95\n","\n","        if 'min_diff_acc' not in args: #Threshold to monitor for\n","            self.args['min_diff_acc'] = 0.001\n","\n","        if 'window_size' not in args:  #Window for monitoring accuracies\n","            self.args['window_size'] = 10\n","            \n","        if 'criterion' not in args:\n","            self.args['criterion'] = nn.CrossEntropyLoss()\n","            \n","        if 'device' not in args:\n","            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        else:\n","            self.device = args['device']\n","\n","    def update_index(self, idxs_lb):\n","        self.idxs_lb = idxs_lb\n","\n","    def update_data(self, new_training_dataset):\n","        \"\"\"\n","        Updates the training dataset with the provided new training dataset\n","        \n","        Parameters\n","        ----------\n","        new_training_dataset: torch.utils.data.Dataset\n","            The new training dataset\n","        \"\"\"\n","        self.training_dataset = AddIndexDataset(new_training_dataset)\n","\n","    def get_acc_on_set(self, test_dataset):\n","        \n","        \"\"\"\n","        Calculates and returns the accuracy on the given dataset to test\n","        \n","        Parameters\n","        ----------\n","        test_dataset: torch.utils.data.Dataset\n","            The dataset to test\n","        Returns\n","        -------\n","        accFinal: float\n","            The fraction of data points whose predictions by the current model match their targets\n","        \"\"\"\t\n","        \n","        try:\n","            self.clf\n","        except:\n","            self.clf = self.net\n","\n","        if test_dataset is None:\n","            raise ValueError(\"Test data not present\")\n","        \n","        if 'batch_size' in self.args:\n","            batch_size = self.args['batch_size']\n","        else:\n","            batch_size = 1 \n","        \n","        loader_te = DataLoader(test_dataset, shuffle=False, pin_memory=True, batch_size=batch_size)\n","        self.clf.eval()\n","        accFinal = 0.\n","\n","        with torch.no_grad():        \n","            self.clf = self.clf.to(device=self.device)\n","            for batch_id, (x,y) in enumerate(loader_te):     \n","                x, y = x.to(device=self.device), y.to(device=self.device)\n","                out = self.clf(x)\n","                accFinal += torch.sum(1.0*(torch.max(out,1)[1] == y)).item() #.data.item()\n","\n","        return accFinal / len(test_dataset)\n","\n","    def _train_weighted(self, epoch, loader_tr, optimizer, gradient_weights):\n","        self.clf.train()\n","        accFinal = 0.\n","        criterion = self.args['criterion']\n","        criterion.reduction = \"none\"\n","\n","        for batch_id, (x, y, idxs) in enumerate(loader_tr):\n","            x, y = x.to(device=self.device), y.to(device=self.device)\n","            gradient_weights = gradient_weights.to(device=self.device)\n","\n","            optimizer.zero_grad()\n","            out = self.clf(x)\n","\n","            # Modify the loss function to apply weights before reducing to a mean\n","            loss = criterion(out, y.long())\n","\n","            # Perform a dot product with the loss vector and the weight vector, then divide by batch size.\n","            weighted_loss = torch.dot(loss, gradient_weights[idxs])\n","            weighted_loss = torch.div(weighted_loss, len(idxs))\n","\n","            accFinal += torch.sum(torch.eq(torch.max(out,1)[1],y)).item() #.data.item()\n","\n","            # Backward now does so on the weighted loss, not the regular mean loss\n","            weighted_loss.backward() \n","\n","            # clamp gradients, just in case\n","            # for p in filter(lambda p: p.grad is not None, self.clf.parameters()): p.grad.data.clamp_(min=-.1, max=.1)\n","\n","            optimizer.step()\n","        return accFinal / len(loader_tr.dataset), weighted_loss\n","\n","    def _train(self, epoch, loader_tr, optimizer):\n","        self.clf.train()\n","        accFinal = 0.\n","        criterion = self.args['criterion']\n","        criterion.reduction = \"mean\"\n","\n","        for batch_id, (x, y, idxs) in enumerate(loader_tr):\n","            x, y = x.to(device=self.device), y.to(device=self.device)\n","\n","            optimizer.zero_grad()\n","            out = self.clf(x)\n","            loss = criterion(out, y.long())\n","            accFinal += torch.sum((torch.max(out,1)[1] == y).float()).item()\n","            loss.backward()\n","\n","            # clamp gradients, just in case\n","            # for p in filter(lambda p: p.grad is not None, self.clf.parameters()): p.grad.data.clamp_(min=-.1, max=.1)\n","\n","            optimizer.step()\n","        return accFinal / len(loader_tr.dataset), loss\n","\n","    def check_saturation(self, acc_monitor):\n","        \n","        saturate = True\n","\n","        for i in range(len(acc_monitor)):\n","            for j in range(i+1, len(acc_monitor)):\n","                if acc_monitor[j] - acc_monitor[i] >= self.args['min_diff_acc']:\n","                    saturate = False\n","                    break\n","\n","        return saturate\n","\n","    def train(self, gradient_weights=None):\n","\n","        print('Training..')\n","        def weight_reset(m):\n","            if hasattr(m, 'reset_parameters'):\n","                m.reset_parameters()\n","\n","        train_logs = []\n","        n_epoch = self.args['n_epoch']\n","        \n","        if self.args['isreset']:\n","            self.clf = self.net.apply(weight_reset).to(device=self.device)\n","        else:\n","            try:\n","                self.clf\n","            except:\n","                self.clf = self.net.apply(weight_reset).to(device=self.device)\n","\n","        if self.args['optimizer'] == 'sgd':\n","            optimizer = optim.SGD(self.clf.parameters(), lr = self.args['lr'], momentum=0.9, weight_decay=5e-4)\n","            lr_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)\n","        \n","        elif self.args['optimizer'] == 'adam':\n","            optimizer = optim.Adam(self.clf.parameters(), lr = self.args['lr'], weight_decay=0)\n","\n","        # ADD stochastic weight averaging\n","        swa_sched = SWALR(optimizer, anneal_strategy=\"cos\", swa_lr=self.args['lr'])\n","        swa_model = AveragedModel(self.clf).to(device=self.device)\n","\n","        if 'batch_size' in self.args:\n","            batch_size = self.args['batch_size']\n","        else:\n","            batch_size = 1\n","\n","        # Set shuffle to true to encourage stochastic behavior for SGD\n","        loader_tr = DataLoader(self.training_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n","        epoch = 1\n","        accCurrent = 0\n","        is_saturated = False\n","        acc_monitor = []\n","        start_swa = False\n","\n","        while (accCurrent < self.args['max_accuracy']) and (epoch < n_epoch) and (not is_saturated): \n","            \n","            if gradient_weights is None:\n","                accCurrent, lossCurrent = self._train(epoch, loader_tr, optimizer)\n","            else:\n","                accCurrent, lossCurrent = self._train_weighted(epoch, loader_tr, optimizer, gradient_weights)\n","            \n","            acc_monitor.append(accCurrent)\n","\n","            if accCurrent > 0.95:\n","                start_swa = True\n","\n","            if start_swa:\n","                swa_model.update_parameters(self.clf)\n","                swa_sched.step()\n","            elif self.args['optimizer'] == 'sgd':\n","                lr_sched.step()\n","\n","            epoch += 1\n","            if(self.args['isverbose']):\n","                if epoch % 50 == 0:\n","                    print(str(epoch) + ' training accuracy: ' + str(accCurrent), flush=True)\n","\n","            #Stop training if not converging\n","            if len(acc_monitor) >= self.args['window_size']:\n","\n","                is_saturated = self.check_saturation(acc_monitor)\n","                del acc_monitor[0]\n","\n","            log_string = 'Epoch:' + str(epoch) + '- training accuracy:'+str(accCurrent)+'- training loss:'+str(lossCurrent)\n","            train_logs.append(log_string)\n","            if (epoch % 50 == 0) and (accCurrent < 0.2): # resetif not converging\n","                self.clf = self.net.apply(weight_reset).to(device=self.device)\n","                \n","                if self.args['optimizer'] == 'sgd':\n","\n","                    optimizer = optim.SGD(self.clf.parameters(), lr = self.args['lr'], momentum=0.9, weight_decay=5e-4)\n","                    lr_sched = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epoch)\n","\n","                else:\n","                    optimizer = optim.Adam(self.clf.parameters(), lr = self.args['lr'], weight_decay=0)\n","\n","        print('Epoch:', str(epoch), 'Training accuracy:', round(accCurrent, 3), flush=True)\n","\n","        # Update batch normalization and set averaged model to clf\n","        update_bn(loader_tr, swa_model, device=self.device)\n","        self.clf = swa_model.module\n","\n","        if self.args['islogs']:\n","            return self.clf, train_logs\n","        else:\n","            return self.clf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8VR-E4Qzm-kG"},"source":["## Shake-Shake Model"]},{"cell_type":"code","metadata":{"id":"jHDOk_kMm-x5"},"source":["class ShakeBlock(nn.Module):\n","\n","    def __init__(self, in_ch, out_ch, stride=1):\n","        super(ShakeBlock, self).__init__()\n","        self.equal_io = in_ch == out_ch\n","        self.shortcut = self.equal_io and None or Shortcut(in_ch, out_ch, stride=stride)\n","\n","        self.branch1 = self._make_branch(in_ch, out_ch, stride)\n","        self.branch2 = self._make_branch(in_ch, out_ch, stride)\n","\n","    def forward(self, x):\n","        h1 = self.branch1(x)\n","        h2 = self.branch2(x)\n","        h = ShakeShake.apply(h1, h2, self.training)\n","        h0 = x if self.equal_io else self.shortcut(x)\n","        return h + h0\n","\n","    def _make_branch(self, in_ch, out_ch, stride=1):\n","        return nn.Sequential(\n","            nn.ReLU(inplace=False),\n","            nn.Conv2d(in_ch, out_ch, 3, padding=1, stride=stride, bias=False),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=False),\n","            nn.Conv2d(out_ch, out_ch, 3, padding=1, stride=1, bias=False),\n","            nn.BatchNorm2d(out_ch))\n","\n","\n","class ShakeResNet(nn.Module):\n","\n","    def __init__(self, depth, w_base, label):\n","        super(ShakeResNet, self).__init__()\n","        n_units = (depth - 2) / 6\n","\n","        in_chs = [16, w_base, w_base * 2, w_base * 4]\n","        self.in_chs = in_chs\n","\n","        self.c_in = nn.Conv2d(3, in_chs[0], 3, padding=1)\n","        self.layer1 = self._make_layer(n_units, in_chs[0], in_chs[1])\n","        self.layer2 = self._make_layer(n_units, in_chs[1], in_chs[2], 2)\n","        self.layer3 = self._make_layer(n_units, in_chs[2], in_chs[3], 2)\n","        self.fc_out = nn.Linear(in_chs[3], label)\n","\n","        # Initialize paramters\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, last=False):\n","        h = self.c_in(x)\n","        h = self.layer1(h)\n","        h = self.layer2(h)\n","        h = self.layer3(h)\n","        h = F.relu(h)\n","        h = F.avg_pool2d(h, 8)\n","        h = h.view(-1, self.in_chs[3])\n","        if last:\n","            output = self.fc_out(h)\n","            return output, h\n","        else:\n","            h = self.fc_out(h)\n","            return h\n","\n","    def get_embedding_dim(self):\n","        return self.fc_out.in_features\n","\n","    def _make_layer(self, n_units, in_ch, out_ch, stride=1):\n","        layers = []\n","        for i in range(int(n_units)):\n","            layers.append(ShakeBlock(in_ch, out_ch, stride=stride))\n","            in_ch, stride = out_ch, 1\n","        return nn.Sequential(*layers)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bd-8vetN6cP-"},"source":["## AL Loop"]},{"cell_type":"code","metadata":{"id":"jWdgKV2M6PFu"},"source":["def train_one(full_train_dataset, initial_train_indices, test_dataset, net, n_rounds, budget, args, nclasses, strategy, save_directory, checkpoint_directory, experiment_name):\n","\n","    # Split the full training dataset into an initial training dataset and an unlabeled dataset\n","    train_dataset = Subset(full_train_dataset, initial_train_indices)\n","    initial_unlabeled_indices = list(set(range(len(full_train_dataset))) - set(initial_train_indices))\n","    unlabeled_dataset = Subset(full_train_dataset, initial_unlabeled_indices)\n","\n","    # Set up the AL strategy\n","    if strategy == \"random\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = RandomSampling(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"entropy\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = EntropySampling(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"margin\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = MarginSampling(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"least_confidence\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = LeastConfidenceSampling(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"badge\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = BADGE(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"coreset\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = CoreSet(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"fass\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = FASS(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"glister\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'lr': args['lr'], 'device':args['device']}\n","        strategy = GLISTER(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args, typeOf='rand', lam=0.1)\n","    elif strategy == \"adversarial_bim\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = AdversarialBIM(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"adversarial_deepfool\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = AdversarialDeepFool(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","    elif strategy == \"bald\":\n","        strategy_args = {'batch_size' : args['batch_size'], 'device':args['device']}\n","        strategy = BALDDropout(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset), net, nclasses, strategy_args)\n","\n","    # Define acc initially\n","    acc = np.zeros(n_rounds+1)\n","\n","    initial_unlabeled_size = len(unlabeled_dataset)\n","\n","    initial_round = 1\n","\n","    # Define an index map\n","    index_map = np.array([x for x in range(initial_unlabeled_size)])\n","\n","    # Attempt to load a checkpoint. If one exists, then the experiment crashed.\n","    training_checkpoint = Checkpoint(experiment_name=experiment_name, path=checkpoint_directory)\n","    rec_acc, rec_indices, rec_state_dict = training_checkpoint.get_saved_values()\n","\n","    # Check if there are values to recover\n","    if rec_acc is not None:\n","\n","        # Restore the accuracy list\n","        for i in range(len(rec_acc)):\n","            acc[i] = rec_acc[i]\n","\n","        # Restore the indices list and shift those unlabeled points to the labeled set.\n","        index_map = np.delete(index_map, rec_indices)\n","\n","        # Record initial size of the training dataset\n","        intial_seed_size = len(train_dataset)\n","\n","        restored_unlabeled_points = Subset(unlabeled_dataset, rec_indices)\n","        train_dataset = ConcatDataset([train_dataset, restored_unlabeled_points])\n","\n","        remaining_unlabeled_indices = list(set(range(len(unlabeled_dataset))) - set(rec_indices))\n","        unlabeled_dataset = Subset(unlabeled_dataset, remaining_unlabeled_indices)\n","\n","        # Restore the model\n","        net.load_state_dict(rec_state_dict) \n","\n","        # Fix the initial round\n","        initial_round = (len(train_dataset) - initial_seed_size) // budget + 1\n","\n","        # Ensure loaded model is moved to GPU\n","        if torch.cuda.is_available():\n","            net = net.cuda()     \n","\n","        strategy.update_model(net)\n","        strategy.update_data(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset)) \n","\n","        dt = data_train(train_dataset, net, args)\n","\n","    else:\n","\n","        if torch.cuda.is_available():\n","            net = net.cuda()\n","\n","        dt = data_train(train_dataset, net, args)\n","\n","        acc[0] = dt.get_acc_on_set(test_dataset)\n","        print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n","\n","        logs = {}\n","        logs['Training Points'] = len(train_dataset)\n","        logs['Test Accuracy'] =  str(round(acc[0]*100, 2))\n","        write_logs(logs, save_directory, 0)\n","          \n","        #Updating the trained model in strategy class\n","        strategy.update_model(net)\n","\n","    # Record the training transform and test transform for disabling purposes\n","    train_transform = full_train_dataset.transform\n","    test_transform = test_dataset.transform\n","\n","    ##User Controlled Loop\n","    for rd in range(initial_round, n_rounds+1):\n","        print('-------------------------------------------------')\n","        print('Round', rd) \n","        print('-------------------------------------------------')\n","\n","        sel_time = time.time()\n","        full_train_dataset.transform = test_transform # Disable any augmentation while selecting points\n","        idx = strategy.select(budget)            \n","        full_train_dataset.transform = train_transform # Re-enable any augmentation done during training\n","        sel_time = time.time() - sel_time\n","        print(\"Selection Time:\", sel_time)\n","\n","        selected_unlabeled_points = Subset(unlabeled_dataset, idx)\n","        train_dataset = ConcatDataset([train_dataset, selected_unlabeled_points])\n","\n","        remaining_unlabeled_indices = list(set(range(len(unlabeled_dataset))) - set(idx))\n","        unlabeled_dataset = Subset(unlabeled_dataset, remaining_unlabeled_indices)\n","\n","        # Update the index map\n","        index_map = np.delete(index_map, idx, axis = 0)\n","\n","        print('Number of training points -', len(train_dataset))\n","\n","        # Start training\n","        strategy.update_data(train_dataset, LabeledToUnlabeledDataset(unlabeled_dataset))\n","        dt.update_data(train_dataset)\n","        t1 = time.time()\n","        clf, train_logs = dt.train(None)\n","        t2 = time.time()\n","        acc[rd] = dt.get_acc_on_set(test_dataset)\n","        logs = {}\n","        logs['Training Points'] = len(train_dataset)\n","        logs['Test Accuracy'] =  str(round(acc[rd]*100, 2))\n","        logs['Selection Time'] = str(sel_time)\n","        logs['Trainining Time'] = str(t2 - t1) \n","        logs['Training'] = train_logs\n","        write_logs(logs, save_directory, rd)\n","        strategy.update_model(clf)\n","        print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n","\n","        # Create a checkpoint\n","        used_indices = np.array([x for x in range(initial_unlabeled_size)])\n","        used_indices = np.delete(used_indices, index_map).tolist()\n","\n","        round_checkpoint = Checkpoint(acc.tolist(), used_indices, clf.state_dict(), experiment_name=experiment_name)\n","        round_checkpoint.save_checkpoint(checkpoint_directory)\n","\n","    print('Training Completed')\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-rFh9y0M3ZVH"},"source":["# CIFAR10"]},{"cell_type":"markdown","metadata":{"id":"E-e_sDnGsC_N"},"source":["## Parameter Definitions\n","\n","Parameters related to the specific experiment are placed here. You should examine each and modify them as needed."]},{"cell_type":"code","metadata":{"id":"0cHXLa_YsIQG"},"source":["data_set_name = \"CIFAR10\" # DSET NAME HERE\n","dataset_root_path = '../downloaded_data/'\n","net = ShakeResNet(depth=14, w_base=32, label=10) # MODEL HERE\n","\n","# MODIFY AS NECESSARY\n","logs_directory = '/content/gdrive/MyDrive/colab_storage/logs/'\n","checkpoint_directory = '/content/gdrive/MyDrive/colab_storage/check/'\n","model_directory = \"/content/gdrive/MyDrive/colab_storage/model/\"\n","\n","experiment_name = \"CIFAR10 SWA SS\"\n","\n","initial_seed_size = 1000 # INIT SEED SIZE HERE\n","training_size_cap = 25000 # TRAIN SIZE CAP HERE\n","\n","budget = 3000 # BUDGET HERE\n","\n","# CHANGE ARGS AS NECESSARY\n","args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':float(0.99), 'islogs':True, 'isreset':True, 'isverbose':True, 'device':'cuda'} \n","\n","# Train on approximately the full dataset given the budget contraints\n","n_rounds = (training_size_cap - initial_seed_size) // budget"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O0WfH3eq3nv_"},"source":["## Initial Loading and Training\n","\n","You may choose to train a new initial model or to continue to load a specific model. If this notebook is being executed in Colab, you should consider whether or not you need the gdown line."]},{"cell_type":"code","metadata":{"id":"K1522SUk3nwF"},"source":["# Mount drive containing possible saved model and define file path.\n","colab_model_storage_mount = \"/content/gdrive\"\n","drive.mount(colab_model_storage_mount)\n","\n","# Retrieve the model from a download link and save it to the drive\n","os.makedirs(logs_directory, exist_ok = True)\n","os.makedirs(checkpoint_directory, exist_ok = True)\n","os.makedirs(model_directory, exist_ok = True)\n","model_directory = F\"{model_directory}/{data_set_name}\"\n","#!/content/gdown.pl/gdown.pl \"INSERT SHARABLE LINK HERE\" \"INSERT DOWNLOAD LOCATION HERE (ideally, same as model_directory)\" # MAY NOT NEED THIS LINE IF NOT CLONING MODEL FROM COLAB\n","\n","# Load the dataset\n","if data_set_name == \"CIFAR10\":\n","\n","    train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n","    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n","\n","    full_train_dataset = datasets.CIFAR10(dataset_root_path, download=True, train=True, transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.CIFAR10(dataset_root_path, download=True, train=False, transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 10 # NUM CLASSES HERE\n","\n","elif data_set_name == \"CIFAR100\":\n","\n","    train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n","    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n","\n","    full_train_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=True, transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.CIFAR100(dataset_root_path, download=True, train=False, transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 100 # NUM CLASSES HERE\n","\n","elif data_set_name == \"MNIST\":\n","\n","    image_dim=28\n","    train_transform = transforms.Compose([transforms.RandomCrop(image_dim, padding=4), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","    test_transform = transforms.Compose([transforms.Resize((image_dim, image_dim)), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","\n","    full_train_dataset = datasets.MNIST(dataset_root_path, download=True, train=True, transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.MNIST(dataset_root_path, download=True, train=False, transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 10 # NUM CLASSES HERE\n","\n","elif data_set_name == \"FashionMNIST\":\n","\n","    train_transform = transforms.Compose([transforms.RandomCrop(28, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]) # Use mean/std of MNIST\n","\n","    full_train_dataset = datasets.FashionMNIST(dataset_root_path, download=True, train=True, transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.FashionMNIST(dataset_root_path, download=True, train=False, transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 10 # NUM CLASSES HERE\n","\n","elif data_set_name == \"SVHN\":\n","\n","    train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n","    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) # ImageNet mean/std\n","\n","    full_train_dataset = datasets.SVHN(dataset_root_path, split='train', download=True, transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.SVHN(dataset_root_path, split='test', download=True, transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 10 # NUM CLASSES HERE\n","\n","elif data_set_name == \"ImageNet\":\n","\n","    train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n","    test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]) # ImageNet mean/std\n","\n","    # Note: Not automatically downloaded due to size restrictions. Notebook needs to be adapted to run on local device.\n","    full_train_dataset = datasets.ImageNet(dataset_root_path, download=False, split='train', transform=train_transform, target_transform=torch.tensor)\n","    test_dataset = datasets.ImageNet(dataset_root_path, download=False, split='val', transform=test_transform, target_transform=torch.tensor)\n","\n","    nclasses = 1000 # NUM CLASSES HERE\n","\n","args['nclasses'] = nclasses\n","\n","dim = full_train_dataset[0][0].shape\n","\n","# Seed the random number generator for reproducibility and create the initial seed set\n","np.random.seed(42)\n","initial_train_indices = np.random.choice(len(full_train_dataset), replace=False, size=initial_seed_size)\n","\n","# COMMENT OUT ONE OR THE OTHER IF YOU WANT TO TRAIN A NEW INITIAL MODEL\n","load_model = False\n","#load_model = True\n","\n","# Only train a new model if one does not exist.\n","if load_model:\n","    net.load_state_dict(torch.load(model_directory))\n","    initial_model = net\n","else:\n","    dt = data_train(Subset(full_train_dataset, initial_train_indices), net, args)\n","    initial_model, _ = dt.train(None)\n","    torch.save(initial_model.state_dict(), model_directory)\n","\n","print(\"Training for\", n_rounds, \"rounds with budget\", budget, \"on unlabeled set size\", training_size_cap)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B9N-4eTMPrZZ"},"source":["## Random Sampling"]},{"cell_type":"code","metadata":{"id":"i4eKSOaiPruO"},"source":["strategy = \"random\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bg1XH87hPsCe"},"source":["## Entropy"]},{"cell_type":"code","metadata":{"id":"mRAKMe2RPsTp"},"source":["strategy = \"entropy\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NkMYHHwyP5Bd"},"source":["## GLISTER"]},{"cell_type":"code","metadata":{"id":"D0KstkZWP5fT"},"source":["strategy = \"glister\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AQmHTnnOP9KU"},"source":["## FASS"]},{"cell_type":"code","metadata":{"id":"tJSuDWowP9fD"},"source":["strategy = \"fass\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ZSiRahu3nwK"},"source":["## BADGE"]},{"cell_type":"code","metadata":{"id":"b5c8AckN3nwK"},"source":["strategy = \"badge\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RQeyff_gWp2E"},"source":["## CoreSet"]},{"cell_type":"code","metadata":{"id":"T4osHfoHWp2F"},"source":["strategy = \"coreset\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TwlszjoBWqMW"},"source":["## Least Confidence"]},{"cell_type":"code","metadata":{"id":"myi_zJ2UWqMW"},"source":["strategy = \"least_confidence\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rv-NjQBFWqWW"},"source":["## Margin"]},{"cell_type":"code","metadata":{"id":"IE0NVJW5WqWW"},"source":["strategy = \"margin\"\n","strat_logs = logs_directory+F'{data_set_name}/{strategy}/'\n","os.makedirs(strat_logs, exist_ok = True)\n","train_one(full_train_dataset, initial_train_indices, test_dataset, copy.deepcopy(initial_model), n_rounds, budget, args, nclasses, strategy, strat_logs, checkpoint_directory, F\"{experiment_name}_{strategy}\")"],"execution_count":null,"outputs":[]}]}